{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THnHILJXB5t5"
      },
      "source": [
        "# Jupyter Notebook: Custom Attention Optimization\n",
        "\n",
        "Description:\n",
        "------------\n",
        "In this notebook, we will:\n",
        "1. Load a pre-built language model (LLM).\n",
        "2. Create a copy of the model architecture but replace its attention mechanism with a simplified one that only attends to the last 5 tokens (instead of all previous tokens).\n",
        "3. Implement a process to compare the outputs of both models and compute a KL-divergence loss.\n",
        "4. Optimize the custom model's parameters by minimizing the KL-divergence between the two modelsâ€™ distributions.\n",
        "5. Demonstrate how to evaluate and compare both models on sample data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MW6t6Oecx_EV",
        "outputId": "f851cb1b-dafc-4fb0-fad5-db9125057ba6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.3.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrmsZrrOk2z7",
        "outputId": "4659e186-3cfe-40de-caa9-1ea411bab195"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.3.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Collecting native-sparse-attention-pytorch\n",
            "  Downloading native_sparse_attention_pytorch-0.2.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: einops>=0.8.1 in /usr/local/lib/python3.11/dist-packages (from native-sparse-attention-pytorch) (0.8.1)\n",
            "Collecting einx>=0.3.0 (from native-sparse-attention-pytorch)\n",
            "  Downloading einx-0.3.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting jaxtyping (from native-sparse-attention-pytorch)\n",
            "  Downloading jaxtyping-0.3.1-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting local-attention>=1.11.1 (from native-sparse-attention-pytorch)\n",
            "  Downloading local_attention-1.11.1-py3-none-any.whl.metadata (907 bytes)\n",
            "Collecting rotary-embedding-torch (from native-sparse-attention-pytorch)\n",
            "  Downloading rotary_embedding_torch-0.8.6-py3-none-any.whl.metadata (675 bytes)\n",
            "Requirement already satisfied: torch>=2.5 in /usr/local/lib/python3.11/dist-packages (from native-sparse-attention-pytorch) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from einx>=0.3.0->native-sparse-attention-pytorch) (2.0.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from einx>=0.3.0->native-sparse-attention-pytorch) (1.13.1)\n",
            "Requirement already satisfied: frozendict in /usr/local/lib/python3.11/dist-packages (from einx>=0.3.0->native-sparse-attention-pytorch) (2.4.6)\n",
            "Collecting hyper-connections>=0.1.8 (from local-attention>=1.11.1->native-sparse-attention-pytorch)\n",
            "  Downloading hyper_connections-0.1.15-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.5->native-sparse-attention-pytorch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5->native-sparse-attention-pytorch) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.5->native-sparse-attention-pytorch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5->native-sparse-attention-pytorch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.5->native-sparse-attention-pytorch) (2024.12.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.5->native-sparse-attention-pytorch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.5->native-sparse-attention-pytorch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.5->native-sparse-attention-pytorch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.5->native-sparse-attention-pytorch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.5->native-sparse-attention-pytorch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.5->native-sparse-attention-pytorch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.5->native-sparse-attention-pytorch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.5->native-sparse-attention-pytorch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.5->native-sparse-attention-pytorch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5->native-sparse-attention-pytorch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5->native-sparse-attention-pytorch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5->native-sparse-attention-pytorch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.5->native-sparse-attention-pytorch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5->native-sparse-attention-pytorch) (3.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->einx>=0.3.0->native-sparse-attention-pytorch) (1.3.0)\n",
            "Collecting wadler-lindig>=0.1.3 (from jaxtyping->native-sparse-attention-pytorch)\n",
            "  Downloading wadler_lindig-0.1.4-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.5->native-sparse-attention-pytorch) (3.0.2)\n",
            "Downloading native_sparse_attention_pytorch-0.2.0-py3-none-any.whl (27 kB)\n",
            "Downloading einx-0.3.0-py3-none-any.whl (102 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m103.0/103.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading local_attention-1.11.1-py3-none-any.whl (9.4 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxtyping-0.3.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55.3/55.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rotary_embedding_torch-0.8.6-py3-none-any.whl (5.6 kB)\n",
            "Downloading hyper_connections-0.1.15-py3-none-any.whl (15 kB)\n",
            "Downloading wadler_lindig-0.1.4-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: wadler-lindig, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jaxtyping, einx, nvidia-cusolver-cu12, rotary-embedding-torch, hyper-connections, local-attention, native-sparse-attention-pytorch\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed einx-0.3.0 hyper-connections-0.1.15 jaxtyping-0.3.1 local-attention-1.11.1 native-sparse-attention-pytorch-0.2.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 rotary-embedding-torch-0.8.6 wadler-lindig-0.1.4\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install native-sparse-attention-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############## # Code Block 1: Imports & Config ##############\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "from native_sparse_attention_pytorch import SparseAttention\n",
        "\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "MODEL_NAME = \"gpt2\"\n",
        "SEQ_LEN = 128\n",
        "BATCH_SIZE = 4\n",
        "NUM_HEADS = 4\n",
        "COMPRESS_RATIO = 0.25\n",
        "WINDOW_SIZE = 64\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "# Native sparse attention configuration\n",
        "SPARSE_CONFIG = {\n",
        "    \"dim\": None,  # Will be set in the model\n",
        "    \"dim_head\": 64,  # Dimension per head\n",
        "    \"heads\": NUM_HEADS,\n",
        "    \"sliding_window_size\": 2,  # Local attention window\n",
        "    \"compress_block_size\": 4,  # Size of blocks to compress\n",
        "    \"selection_block_size\": 4,  # Size of blocks to select from\n",
        "    \"num_selected_blocks\": 2,  # Number of blocks to select\n",
        "}"
      ],
      "metadata": {
        "id": "bZD0g6sNPoaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############## # Code Block 2: Sparse Attention Components ##############\n",
        "class CompressedGlobalAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, compress_ratio):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.compress_ratio = compress_ratio\n",
        "\n",
        "        self.Wq = nn.Linear(embed_dim, embed_dim)\n",
        "        self.Wk = nn.Linear(embed_dim, embed_dim)\n",
        "        self.Wv = nn.Linear(embed_dim, embed_dim)\n",
        "        self.compression = nn.Linear(embed_dim, 1)\n",
        "        self.expansion = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x, attention_mask=None):\n",
        "        B, T, D = x.shape\n",
        "        keep_num = max(1, int(T * self.compress_ratio))\n",
        "\n",
        "        # Token compression\n",
        "        importance = self.compression(x).squeeze(-1)\n",
        "        _, keep_idx = torch.topk(importance, k=keep_num, dim=-1)\n",
        "        x_compressed = torch.gather(x, 1, keep_idx.unsqueeze(-1).expand(-1, -1, D))\n",
        "\n",
        "        # Projections\n",
        "        Q = self.Wq(x).view(B, T, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = (\n",
        "            self.Wk(x_compressed)\n",
        "            .view(B, keep_num, self.num_heads, self.head_dim)\n",
        "            .permute(0, 2, 1, 3)\n",
        "        )\n",
        "        V = (\n",
        "            self.Wv(x_compressed)\n",
        "            .view(B, keep_num, self.num_heads, self.head_dim)\n",
        "            .permute(0, 2, 1, 3)\n",
        "        )\n",
        "\n",
        "        # Attention\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-1, -2)) / math.sqrt(self.head_dim)\n",
        "\n",
        "        # Masking\n",
        "        if attention_mask is not None:\n",
        "            compressed_mask = torch.gather(attention_mask, 1, keep_idx)\n",
        "            attn_scores = attn_scores.masked_fill(\n",
        "                compressed_mask.unsqueeze(1).unsqueeze(2) == 0, -1e10\n",
        "            )\n",
        "\n",
        "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
        "        output = torch.matmul(attn_weights, V)\n",
        "        output = output.permute(0, 2, 1, 3).contiguous().view(B, T, D)\n",
        "\n",
        "        output = self.expansion(output)  # Ensure output has correct embedding dimension\n",
        "        output = output[:, : x.size(1), :]\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class LocalWindowAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, window_size):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.window_size = window_size\n",
        "\n",
        "        self.Wq = nn.Linear(embed_dim, embed_dim)\n",
        "        self.Wk = nn.Linear(embed_dim, embed_dim)\n",
        "        self.Wv = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def create_window_mask(self, seq_len, device):\n",
        "        mask = torch.zeros(seq_len, seq_len, device=device)\n",
        "        for i in range(seq_len):\n",
        "            start = max(0, i - self.window_size // 2)\n",
        "            end = min(seq_len, i + self.window_size // 2 + 1)\n",
        "            mask[i, start:end] = 1\n",
        "        return mask.unsqueeze(0).unsqueeze(1)\n",
        "\n",
        "    def forward(self, x, attention_mask=None):\n",
        "        B, T, D = x.shape\n",
        "        window_mask = self.create_window_mask(T, x.device)\n",
        "\n",
        "        Q = self.Wq(x).view(B, T, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = self.Wk(x).view(B, T, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = self.Wv(x).view(B, T, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-1, -2)) / math.sqrt(self.head_dim)\n",
        "        attn_scores = attn_scores.masked_fill(window_mask == 0, -1e10)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(\n",
        "                attention_mask.unsqueeze(1).unsqueeze(2) == 0, -1e10\n",
        "            )\n",
        "\n",
        "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
        "        output = torch.matmul(attn_weights, V)\n",
        "        return output.permute(0, 2, 1, 3).contiguous().view(B, T, D)\n",
        "\n",
        "\n",
        "class HierarchicalSparseAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, window_size, compress_ratio):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads  # ğŸ”¹ Store num_heads\n",
        "        self.local_attn = LocalWindowAttention(embed_dim, num_heads, window_size)\n",
        "        self.global_attn = CompressedGlobalAttention(\n",
        "            embed_dim, num_heads, compress_ratio\n",
        "        )\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.Linear(\n",
        "                embed_dim, num_heads * 2\n",
        "            ),  # Ensure output is [batch, seq_len, num_heads * 2]\n",
        "            nn.Softmax(dim=-1),\n",
        "        )\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x, attention_mask=None):\n",
        "        # Get outputs from local and global attention modules.\n",
        "        local_out = self.local_attn(x, attention_mask)  # Expected shape: (B, T, D)\n",
        "        global_out = self.global_attn(x, attention_mask)  # Expected shape: (B, T, D)\n",
        "\n",
        "        B, T, D = x.size()\n",
        "        head_dim = D // self.num_heads  # Ensure D is divisible by num_heads\n",
        "\n",
        "        # Compute gating weights.\n",
        "        # self.gate should output a tensor of shape (B, T, num_heads*2)\n",
        "        gate_out = self.gate(x)  # Shape: (B, T, num_heads*2)\n",
        "        # Reshape to (B, T, num_heads, 2) where last dim holds [local_gate, global_gate]\n",
        "        gates = gate_out.view(B, T, self.num_heads, 2)\n",
        "        # Unbind the last dimension into two tensors\n",
        "        local_gate = gates[..., 0]  # Shape: (B, T, num_heads)\n",
        "        global_gate = gates[..., 1]  # Shape: (B, T, num_heads)\n",
        "\n",
        "        # Reshape attention outputs to split heads: (B, T, num_heads, head_dim)\n",
        "        local_out_heads = local_out.view(B, T, self.num_heads, head_dim)\n",
        "        global_out_heads = global_out.view(B, T, self.num_heads, head_dim)\n",
        "\n",
        "        # Ensure the gate tensors have an extra dimension for broadcasting: (B, T, num_heads, 1)\n",
        "        local_gate = local_gate.unsqueeze(-1)\n",
        "        global_gate = global_gate.unsqueeze(-1)\n",
        "\n",
        "        # Element-wise multiply each head output by its corresponding gate weight\n",
        "        combined = local_out_heads * local_gate + global_out_heads * global_gate\n",
        "        # Reshape back to (B, T, D)\n",
        "        combined = combined.view(B, T, D)\n",
        "        return self.out_proj(combined)"
      ],
      "metadata": {
        "id": "LJD24-_sPl5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############## # Code Block 3: Custom GPT-2 Model ##############\n",
        "class SparseGPT2(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.wte = nn.Embedding(config.vocab_size, config.hidden_size)\n",
        "        self.wpe = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "        self.drop = nn.Dropout(config.embd_pdrop)\n",
        "\n",
        "        # Create native sparse attention layer with correct parameters\n",
        "        sparse_config = SPARSE_CONFIG.copy()\n",
        "        sparse_config[\"dim\"] = config.hidden_size\n",
        "        sparse_config[\"compress_block_sliding_stride\"] = 2\n",
        "        self.sparse_attn = SparseAttention(**sparse_config)\n",
        "\n",
        "        self.h = nn.ModuleList(\n",
        "            [\n",
        "                nn.ModuleDict(\n",
        "                    {\n",
        "                        \"attn\": self.sparse_attn,\n",
        "                        \"ln_1\": nn.LayerNorm(config.hidden_size),\n",
        "                        \"mlp\": nn.Sequential(\n",
        "                            nn.Linear(config.hidden_size, 4 * config.hidden_size),\n",
        "                            nn.GELU(),\n",
        "                            nn.Linear(4 * config.hidden_size, config.hidden_size),\n",
        "                        ),\n",
        "                        \"ln_2\": nn.LayerNorm(config.hidden_size),\n",
        "                    }\n",
        "                )\n",
        "                for _ in range(config.num_hidden_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.ln_f = nn.LayerNorm(config.hidden_size)\n",
        "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        B, T = input_ids.size()\n",
        "        pos_ids = torch.arange(T, device=DEVICE).unsqueeze(0)\n",
        "\n",
        "        x = self.drop(self.wte(input_ids) + self.wpe(pos_ids))\n",
        "\n",
        "        attn_out = torch.zeros_like(x)\n",
        "\n",
        "        for block in self.h:\n",
        "            # Apply layer norm before attention\n",
        "            normed_x = block[\"ln_1\"](x)\n",
        "            attention_result = block[\"attn\"](normed_x)\n",
        "\n",
        "            # Apply sparse attention and handle tuple output\n",
        "            # attn_output = block[\"attn\"](normed_x)\n",
        "            if attention_result is not None:\n",
        "                attn_out = attention_result\n",
        "            # # If attn_output is a tuple, take the first element (the main output)\n",
        "            # if isinstance(attn_output, tuple):\n",
        "            #     attn_out = attn_output[0]\n",
        "            # else:\n",
        "            #     attn_out = attn_output\n",
        "\n",
        "            # Apply mask after attention if provided\n",
        "            if attention_mask is not None:\n",
        "                attn_out = attn_out * attention_mask.unsqueeze(-1)\n",
        "\n",
        "            x = x + attn_out\n",
        "            x = x + block[\"mlp\"](block[\"ln_2\"](x))\n",
        "\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "        return logits  # Return only the logits, not a tuple"
      ],
      "metadata": {
        "id": "ALL7oG_SPikQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############## # Code Block 4: Training Setup ##############\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Reference model\n",
        "ref_model = GPT2LMHeadModel.from_pretrained(MODEL_NAME).to(DEVICE)\n",
        "ref_model.eval()\n",
        "\n",
        "# Custom model with native sparse attention\n",
        "cust_config = GPT2Config.from_pretrained(MODEL_NAME)\n",
        "cust_model = SparseGPT2(cust_config).to(DEVICE)\n",
        "\n",
        "# Initialize with pretrained weights\n",
        "pretrained_state_dict = ref_model.state_dict()\n",
        "cust_model.load_state_dict(pretrained_state_dict, strict=False)\n",
        "\n",
        "# Use a lower learning rate for fine-tuning\n",
        "optimizer = torch.optim.AdamW(cust_model.parameters(), lr=5e-5)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)"
      ],
      "metadata": {
        "id": "03IV1qpEPftj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############## # Code Block 5: Training Loop ##############\n",
        "def train_step(batch):\n",
        "    inputs = batch.to(DEVICE)\n",
        "    attention_mask = (inputs != tokenizer.pad_token_id).float()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        ref_outputs = ref_model(inputs, attention_mask=attention_mask)\n",
        "        if isinstance(ref_outputs, tuple):\n",
        "            ref_logits = ref_outputs[0]\n",
        "        else:\n",
        "            ref_logits = ref_outputs.logits  # Extract logits from the output object\n",
        "\n",
        "    cust_outputs = cust_model(inputs, attention_mask=attention_mask)\n",
        "    if isinstance(cust_outputs, tuple):\n",
        "        cust_logits = cust_outputs[0]\n",
        "    else:\n",
        "        cust_logits = cust_outputs\n",
        "\n",
        "    # Use KL divergence loss with temperature\n",
        "    temperature = 1.0\n",
        "    loss = F.kl_div(\n",
        "        F.log_softmax(cust_logits / temperature, dim=-1),\n",
        "        F.softmax(ref_logits / temperature, dim=-1).detach(),\n",
        "        reduction=\"batchmean\",\n",
        "    ) * (temperature**2)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(cust_model.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "def train_epoch(loader):\n",
        "    cust_model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(loader, desc=\"Training\"):\n",
        "        loss = train_step(batch)\n",
        "        total_loss += loss\n",
        "    return total_loss / len(loader)"
      ],
      "metadata": {
        "id": "t5EVt0dWPbvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############## # Code Block 6: Generation & Evaluation ##############\n",
        "def generate(model, tokenizer, prompt, max_length=50, temperature=0.7, top_k=50, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=False).to(device)\n",
        "    input_ids = input_ids[:1]  # Ensure we only have one batch dimension\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids)\n",
        "            logits = outputs.logits if hasattr(outputs, \"logits\") else outputs\n",
        "\n",
        "            # Get logits for the last token\n",
        "            if logits.dim() == 3:  # Standard shape [batch, seq, vocab]\n",
        "                next_token_logits = logits[:, -1, :] / temperature\n",
        "            elif logits.dim() == 2:  # If somehow we got [batch*seq, vocab]\n",
        "                # We only care about the last token's logits\n",
        "                next_token_logits = logits[-1:, :] / temperature\n",
        "            else:\n",
        "                raise ValueError(f\"Unexpected logits shape: {logits.shape}\")\n",
        "\n",
        "            # Apply top-k filtering\n",
        "            top_k_logits, top_k_indices = torch.topk(next_token_logits, k=top_k, dim=-1)\n",
        "\n",
        "            # Convert to probabilities\n",
        "            probs = F.softmax(top_k_logits, dim=-1)\n",
        "\n",
        "            # Sample next token index from top-k logits\n",
        "            next_token_idx = torch.multinomial(probs, num_samples=1)\n",
        "            next_token = top_k_indices.gather(1, next_token_idx)\n",
        "\n",
        "            # Ensure next_token has shape [1, 1]\n",
        "            next_token = next_token[-1:, :]  # Take only the last row if needed\n",
        "\n",
        "            # Concatenate to input_ids\n",
        "            input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
        "\n",
        "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "BWnppj1QPYEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############## # Code Block 7: Dataset Preparation ##############\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "class WikiDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, seq_len):\n",
        "        self.samples = []\n",
        "        for text in texts:\n",
        "            # Tokenize each text separately, without adding special tokens\n",
        "            token_ids = tokenizer.encode(text, add_special_tokens=False)\n",
        "            # Split token_ids into chunks of length seq_len\n",
        "            for i in range(0, len(token_ids), seq_len):\n",
        "                chunk = token_ids[i : i + seq_len]\n",
        "                # Only add full chunks to avoid very short sequences\n",
        "                if len(chunk) == seq_len:\n",
        "                    self.samples.append(torch.tensor(chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx]\n",
        "\n",
        "\n",
        "# Load a small subset (e.g., 1% of the train split) of WikiText data\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train[:80%]\")\n",
        "texts = dataset[\"text\"]\n",
        "wiki_dataset = WikiDataset(texts, tokenizer, SEQ_LEN)\n",
        "\n",
        "# Create a DataLoader for training\n",
        "train_loader = DataLoader(wiki_dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "id": "sAWHHuaTPUd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0d4Q3hFk1ey",
        "outputId": "5677661c-95a7-4d14-dd8e-5812e6a64444"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2036/2036 [08:34<00:00,  3.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 - Average Loss: 331.6650\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2036/2036 [08:37<00:00,  3.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5 - Average Loss: 242.4077\n",
            "\n",
            "Generating sample text:\n",
            "Reference: Artificial intelligence has always been a topic of debate. Many of these debates have been about how to make AI intelligible. The problem is that AI cannot understand abstract concepts. The only way to have a very good understanding of abstract concepts is to have basic knowledge of\n",
            "Custom: Artificial intelligence \"I \" to write a \"the, \" is thought too \" or \" is \" a \" \" \" I \" \" \" \" \" \"\" \" \" \" \" \" \"We \" \" \" \" \"the \" \" \" \" \" \" \"\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2036/2036 [08:36<00:00,  3.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5 - Average Loss: 207.6536\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2036/2036 [08:35<00:00,  3.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5 - Average Loss: 186.9719\n",
            "\n",
            "Generating sample text:\n",
            "Reference: Artificial intelligence may not necessarily be a good thing for the American public.\n",
            "\n",
            "As we've reported elsewhere, there are many positive benefits to AI for the public sector.\n",
            "\n",
            "For example, many people are happy about the way AI works.\n",
            "\n",
            "Many\n",
            "Custom: Artificial intelligence to say a new standard decision to keep their \"to-pamarimimimimz, S.Nxdbhhhar's , and (in , S.G. , and (M , and M.ANN\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2036/2036 [08:36<00:00,  3.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5 - Average Loss: 175.6799\n",
            "\n",
            "Final generation comparison:\n",
            "Reference: Artificial intelligence is the key to the future, but it may also be the key to the future with its ability to detect, investigate and manage complex patterns of action.\n",
            "\n",
            "Some scientists, for example, have proposed that artificial intelligence could be the next big thing\n",
            "Custom: Artificial intelligence,, will not pay those that they were the most common people could be understood with other other other two-tetetetetetetetetetetetetetetetetetetetetetetetetetetete\n"
          ]
        }
      ],
      "source": [
        "############## # Code Block 8: Training Execution ##############\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    avg_loss = train_epoch(train_loader)\n",
        "    scheduler.step()\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} - Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Generate sample text after each epoch\n",
        "    if (epoch + 1) % 2 == 0:  # Generate every 2 epochs\n",
        "        print(\"\\nGenerating sample text:\")\n",
        "        prompt = \"Artificial intelligence\"\n",
        "        print(\"Reference:\", generate(ref_model, tokenizer, prompt, temperature=0.7, top_k=50))\n",
        "        print(\"Custom:\", generate(cust_model, tokenizer, prompt, temperature=0.7, top_k=50))\n",
        "        print(\"\\n\")\n",
        "\n",
        "# Final generation comparison\n",
        "prompt = \"Artificial intelligence\"\n",
        "print(\"\\nFinal generation comparison:\")\n",
        "print(\"Reference:\", generate(ref_model, tokenizer, prompt, temperature=0.7, top_k=50))\n",
        "print(\"Custom:\", generate(cust_model, tokenizer, prompt, temperature=0.7, top_k=50))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad7z6_03TOs2"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "We have demonstrated:\n",
        "1) Loading a reference GPT-2 model from Hugging Face.\n",
        "2) Creating a custom GPT-2-like model with a simplified NSA attention mechanism.\n",
        "3) Setting up a dataset and training loop that optimizes the custom model to match the reference distribution via KL-divergence.\n",
        "4) Showed a simple comparison of generated text from both models.\n",
        "\n",
        "This notebook is purely for demonstration and educational purposes, and many improvements could be made:\n",
        "- More elaborate data loading\n",
        "- Proper scheduling, regularization\n",
        "- Additional GPT-2 intricacies (like caching attention states, etc.)\n",
        "- More advanced generation strategies (beam search, top-k, top-p, etc.)\n",
        "\n",
        "But this entire workflow shows how one could begin to experiment with custom attention\n",
        "mechanisms and align them to a known distribution via KL divergence."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}