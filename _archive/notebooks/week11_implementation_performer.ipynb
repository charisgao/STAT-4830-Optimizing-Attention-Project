{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# replaced GPT2 attention blocks with \"Performer\" attention blocks (uses random feature approximation instead of the standard quadratic attention)\n",
        "# training parameters optimized are weights within the PerformerAttention module: query, key, value projection weights/biases + output weights/biases\n",
        "  # selectively enabling gradient updates only for parameters in the attention layers\n",
        "\n",
        "# optimization process\n",
        "# run both models on the same input\n",
        "# compute KL divergence between their outputs\n",
        "# backpropagate through the custom model to update only the attention parameters\n",
        "# repeat until convergence"
      ],
      "metadata": {
        "id": "P5QyWCJvLyM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDCY3kPtBHjr",
        "outputId": "83e75e10-ec4f-41be-a9fa-4f4acec69fc9",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.29.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    GPT2LMHeadModel,\n",
        "    GPT2Config\n",
        ")\n",
        "\n",
        "from transformers.models.gpt2.modeling_gpt2 import GPT2Block"
      ],
      "metadata": {
        "id": "WH3A46QFBPHW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PerformerAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    A minimal Performer-style Multi-Head Attention module using random feature maps.\n",
        "    - d_model: total embedding dimension\n",
        "    - num_heads: number of attention heads\n",
        "    - n_features: number of random features (sometimes denoted as 'r' or 'm')\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads=8, n_features=256):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.n_features = n_features\n",
        "\n",
        "        # Q, K, V projections\n",
        "        self.query_proj = nn.Linear(d_model, d_model)\n",
        "        self.key_proj = nn.Linear(d_model, d_model)\n",
        "        self.value_proj = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Output projection\n",
        "        self.out_proj = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # We create random weights for the feature maps.\n",
        "        # shape: (num_heads, head_dim, n_features)\n",
        "        # Typically Gaussian for the 'favor+' trick.\n",
        "        self.register_buffer(\n",
        "            \"proj_matrix\",\n",
        "            torch.randn(self.num_heads, self.head_dim, self.n_features)\n",
        "        )\n",
        "        # Optionally a random bias\n",
        "        self.register_buffer(\n",
        "            \"proj_bias\",\n",
        "            2 * torch.pi * torch.rand(self.num_heads, self.n_features)\n",
        "        )\n",
        "\n",
        "        # Constants for numerical stability\n",
        "        self.EPS = 1e-8\n",
        "        self.EPS_NORM = 1e-5\n",
        "        self.MAX_CLIP = 1e3\n",
        "        self.FEATURE_SCALE = 1.0 / math.sqrt(2 * self.n_features)\n",
        "\n",
        "    def forward(self, x, attention_mask=None):\n",
        "        \"\"\"\n",
        "        x: (batch_size, seq_len, d_model)\n",
        "        attention_mask: optional; shape (batch_size, seq_len) or broadcastable to (B, 1, L)\n",
        "        1 for valid tokens, 0 for masked.\n",
        "        Returns: (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        # Replace extreme values right at the start\n",
        "        # x = torch.nan_to_num(x, nan=0.0, posinf=self.MAX_CLIP, neginf=-self.MAX_CLIP)\n",
        "        # x = torch.clamp(x, -self.MAX_CLIP, self.MAX_CLIP)\n",
        "\n",
        "        B, L, D = x.shape\n",
        "\n",
        "        # 1. Project to Q, K, V\n",
        "        q = self.query_proj(x)  # (B, L, d_model)\n",
        "        k = self.key_proj(x)\n",
        "        v = self.value_proj(x)\n",
        "\n",
        "        # Replace extreme values after projection\n",
        "        # q = torch.nan_to_num(q, nan=0.0, posinf=self.MAX_CLIP, neginf=-self.MAX_CLIP)\n",
        "        # k = torch.nan_to_num(k, nan=0.0, posinf=self.MAX_CLIP, neginf=-self.MAX_CLIP)\n",
        "        # v = torch.nan_to_num(v, nan=0.0, posinf=self.MAX_CLIP, neginf=-self.MAX_CLIP)\n",
        "\n",
        "        # 2. Reshape into multiple heads\n",
        "        # (B, L, num_heads, head_dim) => then transpose to (B, num_heads, L, head_dim)\n",
        "        q = q.view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        k = k.view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        v = v.view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # # Scale Q and K if norms are too large (using more stable approach)\n",
        "        # q_norm = torch.norm(q, dim=-1, keepdim=True).clamp_min(self.EPS_NORM)\n",
        "        # k_norm = torch.norm(k, dim=-1, keepdim=True).clamp_min(self.EPS_NORM)\n",
        "\n",
        "        # max_norm = 5.0\n",
        "        # q_norm_factor = torch.where(q_norm > max_norm, max_norm / q_norm, torch.ones_like(q_norm))\n",
        "        # k_norm_factor = torch.where(k_norm > max_norm, max_norm / k_norm, torch.ones_like(k_norm))\n",
        "\n",
        "        # q = q * q_norm_factor\n",
        "        # k = k * k_norm_factor\n",
        "\n",
        "        q = q / math.sqrt(self.head_dim)\n",
        "        k = k / math.sqrt(self.head_dim)\n",
        "\n",
        "        # 3. Convert Q, K to random feature space\n",
        "        q_features = self.random_feature_map(q)  # (B, H, L, n_features)\n",
        "        k_features = self.random_feature_map(k)  # (B, H, L, n_features)\n",
        "\n",
        "        # Handle attention mask\n",
        "        if attention_mask is not None:\n",
        "            # Simplify to just apply the mask to k_features\n",
        "            # attention_mask = attention_mask.unsqueeze(1).unsqueeze(-1)  # (B, 1, L, 1)\n",
        "            # k_features = k_features * attention_mask\n",
        "\n",
        "            # Handle attention mask based on its dimensionality\n",
        "            if attention_mask.dim() == 2:  # (B, L)\n",
        "                # Convert to 3D mask\n",
        "                attention_mask = attention_mask.unsqueeze(1)  # (B, 1, L)\n",
        "            elif attention_mask.dim() == 4:  # (B, 1, L, L)\n",
        "                # Convert to (B, 1, L) by checking if there are any valid keys for each query\n",
        "                attention_mask = (attention_mask.sum(dim=-1) > 0).float()  # (B, 1, L)\n",
        "\n",
        "            # Apply mask to k_features and v (expanded for broadcasting)\n",
        "            mask_4d = attention_mask.unsqueeze(-1)  # (B, 1, L, 1)\n",
        "            k_features = k_features * mask_4d\n",
        "\n",
        "        # 4. Compute linear attention\n",
        "        # Key step: k_features.sum(dim=2) computes the normalization factor\n",
        "        kv = torch.einsum(\"bhlf,bhld->bhfd\", k_features, v)\n",
        "        k_sum = k_features.sum(dim=2)  # (B, H, n_features)\n",
        "\n",
        "        # 5. Apply single small epsilon in the denominator\n",
        "        k_sum = k_sum + 1e-6\n",
        "\n",
        "        # 6. Compute attention output\n",
        "        numerator = torch.einsum(\"bhlf,bhfd->bhld\", q_features, kv)\n",
        "        denominator = torch.einsum(\"bhlf,bhf->bhl\", q_features, k_sum).unsqueeze(-1)\n",
        "        out = numerator / denominator  # (B, H, L, head_dim)\n",
        "\n",
        "        # Final safety check\n",
        "        # out = torch.nan_to_num(out, nan=0.0, posinf=self.MAX_CLIP, neginf=-self.MAX_CLIP)\n",
        "        # out = torch.clamp(out, -self.MAX_CLIP, self.MAX_CLIP)\n",
        "\n",
        "        # 8. Recombine heads\n",
        "        out = out.transpose(1, 2).contiguous().view(B, L, self.num_heads * self.head_dim)\n",
        "        out = self.out_proj(out)\n",
        "\n",
        "        # Final check - replace any remaining NaNs or Infs\n",
        "        # out = torch.nan_to_num(out, nan=0.0, posinf=self.MAX_CLIP, neginf=-self.MAX_CLIP)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def random_feature_map(self, x):\n",
        "      \"\"\"\n",
        "      FAVOR+ implementation that properly approximates softmax attention\n",
        "      x: (B, H, L, head_dim)\n",
        "      \"\"\"\n",
        "      # 1. Normalize x for numerical stability\n",
        "      x_norm = x / math.sqrt(self.head_dim)\n",
        "\n",
        "      # 2. Project normalized vectors (using orthogonal random features if possible)\n",
        "      proj = torch.einsum(\"bhld,hdf->bhlf\", x_norm, self.proj_matrix)\n",
        "\n",
        "      # 3. Add random bias\n",
        "      proj = proj + self.proj_bias.unsqueeze(0).unsqueeze(2)\n",
        "\n",
        "      # 4. Apply non-linearity: exp(x) / sqrt(m) for positive features\n",
        "      # This properly approximates softmax attention\n",
        "      exp_proj = torch.exp(proj)\n",
        "\n",
        "      # 5. Create concatenated features\n",
        "      # The 1/sqrt(m) scaling is crucial for approximation accuracy\n",
        "      out = exp_proj / math.sqrt(self.n_features)\n",
        "\n",
        "      return out"
      ],
      "metadata": {
        "id": "fsdVDynCBPzF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# Custom GPT-2 Block with our CustomLearnableAttention\n",
        "# ------------------------------\n",
        "class CustomGPT2Block(GPT2Block):\n",
        "    \"\"\"\n",
        "    Subclass the original GPT2Block to replace the attention layer with our custom one.\n",
        "    We also copy the original Q, K, V, and output projection weights so that the custom\n",
        "    attention begins with a similar behavior to the baseline.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.attn = self._create_custom_attention(config) # replace default attention w/ custom attention\n",
        "\n",
        "    def _create_custom_attention(self, config):\n",
        "        original_attn = self.attn\n",
        "\n",
        "        custom_attn = PerformerAttention(\n",
        "          d_model=config.n_embd,\n",
        "          num_heads=config.n_head,\n",
        "          n_features=256\n",
        "        )\n",
        "\n",
        "        # Instead of copying weights directly, initialize them with a custom scheme\n",
        "        with torch.no_grad():\n",
        "            # Scale the random projections properly for good approximation\n",
        "            nn.init.orthogonal_(custom_attn.proj_matrix)\n",
        "\n",
        "            # Get original weights (but apply a scale factor)\n",
        "            old_w = original_attn.c_attn.weight.T\n",
        "            old_b = original_attn.c_attn.bias\n",
        "\n",
        "            # Split Q, K, V\n",
        "            q_w, k_w, v_w = old_w.chunk(3, dim=0)\n",
        "            q_b, k_b, v_b = old_b.chunk(3, dim=0)\n",
        "\n",
        "            # Apply a scaling factor to account for the different mechanism\n",
        "            scale = 1.0 / math.sqrt(config.n_head)\n",
        "\n",
        "            # Copy with scaling\n",
        "            custom_attn.query_proj.weight.copy_(q_w * scale)\n",
        "            custom_attn.query_proj.bias.copy_(q_b * scale)\n",
        "            custom_attn.key_proj.weight.copy_(k_w * scale)\n",
        "            custom_attn.key_proj.bias.copy_(k_b * scale)\n",
        "\n",
        "            # Value projections can be copied directly\n",
        "            custom_attn.value_proj.weight.copy_(v_w)\n",
        "            custom_attn.value_proj.bias.copy_(v_b)\n",
        "\n",
        "            # Output projection can be copied directly\n",
        "            custom_attn.out_proj.load_state_dict(original_attn.c_proj.state_dict())\n",
        "\n",
        "        return custom_attn\n",
        "\n",
        "        # # We can copy part of the old c_attn weights (which was a big linear that had Q,K,V).\n",
        "        # with torch.no_grad():\n",
        "        #     old_w = original_attn.c_attn.weight # 768, 2304\n",
        "        #     old_b = original_attn.c_attn.bias # 2304\n",
        "\n",
        "        #     old_w = old_w.T\n",
        "\n",
        "        #     # old_w has shape (3 * d_model, d_model), corresponding to Q, K, V stacked.\n",
        "        #     q_w, k_w, v_w = old_w.chunk(3, dim=0)\n",
        "        #     q_b, k_b, v_b = old_b.chunk(3, dim=0)\n",
        "\n",
        "        #     # Copy into the new Q, K, V\n",
        "        #     custom_attn.query_proj.weight.copy_(q_w)\n",
        "        #     custom_attn.query_proj.bias.copy_(q_b)\n",
        "        #     custom_attn.key_proj.weight.copy_(k_w)\n",
        "        #     custom_attn.key_proj.bias.copy_(k_b)\n",
        "        #     custom_attn.value_proj.weight.copy_(v_w)\n",
        "        #     custom_attn.value_proj.bias.copy_(v_b)\n",
        "\n",
        "        #     # c_proj is the final linear after attention; we can copy that to our out_proj\n",
        "        #     custom_attn.out_proj.load_state_dict(original_attn.c_proj.state_dict())\n",
        "\n",
        "        # return custom_attn\n",
        "\n",
        "    def forward(self, hidden_states, layer_past=None, attention_mask=None,\n",
        "                head_mask=None, use_cache=False, output_attentions=False, **kwargs):\n",
        "        # Standard GPT-2 block forward pass with two residual connections.\n",
        "        attn_input = self.ln_1(hidden_states)\n",
        "        attn_output = self.attn(attn_input, attention_mask=attention_mask)\n",
        "        hidden_states = hidden_states + attn_output\n",
        "\n",
        "        mlp_input = self.ln_2(hidden_states)\n",
        "        mlp_output = self.mlp(mlp_input)\n",
        "        hidden_states = hidden_states + mlp_output\n",
        "\n",
        "        # We do not use caching in our custom model.\n",
        "        return (hidden_states, None, None)"
      ],
      "metadata": {
        "id": "9yk8q5yJBP-0"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# Custom GPT-2 LM Model that Uses the Custom Blocks\n",
        "# ------------------------------\n",
        "class CustomGPT2LM(GPT2LMHeadModel):\n",
        "    \"\"\"\n",
        "    This custom language model replaces each Transformer block with our custom block\n",
        "    (which uses the learnable attention mask). In addition, we share the word and\n",
        "    positional embeddings, as well as the LM head, with the reference model.\n",
        "    \"\"\"\n",
        "    def __init__(self, config, reference_model):\n",
        "        super().__init__(config)\n",
        "        # Disable caching for generation.\n",
        "        self.config.use_cache = False\n",
        "\n",
        "        # Share embeddings and the language model head from the reference model.\n",
        "        self.transformer.wte = reference_model.transformer.wte\n",
        "        self.transformer.wpe = reference_model.transformer.wpe\n",
        "        self.lm_head = reference_model.lm_head\n",
        "\n",
        "        # Replace all Transformer blocks with our custom blocks.\n",
        "        self.transformer.h = nn.ModuleList([CustomGPT2Block(config) for _ in range(config.n_layer)])\n",
        "        # Load weights from the reference model (allowing missing keys since our modules are modified).\n",
        "        self.load_state_dict(reference_model.state_dict(), strict=False)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, **kwargs):\n",
        "        # If a 4D mask is passed in (e.g. from GPT‑2 generation), replace it with a 2D mask.\n",
        "        if attention_mask is not None and attention_mask.dim() == 4:\n",
        "            attention_mask = (input_ids != tokenizer.pad_token_id).long()\n",
        "        return super().forward(input_ids=input_ids, attention_mask=attention_mask, **kwargs)"
      ],
      "metadata": {
        "id": "oST_HBmkBQWy"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# Prepare the Dataset and Dataloader using wikitext-2\n",
        "# ------------------------------\n",
        "# We create a simple Dataset that tokenizes the raw texts from wikitext.\n",
        "class WikiTextDataset(Dataset):\n",
        "    def __init__(self, tokenizer, texts, max_length=32):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.texts = texts\n",
        "        self.encodings = []\n",
        "\n",
        "        for txt in texts:\n",
        "            # Tokenize each text and pad/truncate to max_length.\n",
        "            enc = tokenizer.encode_plus(\n",
        "                txt,\n",
        "                max_length=self.max_length,\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "            self.encodings.append(enc)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        enc = self.encodings[idx]\n",
        "        input_ids = enc[\"input_ids\"].squeeze(0)         # shape: (max_length,)\n",
        "        attention_mask = enc[\"attention_mask\"].squeeze(0)   # shape: (max_length,)\n",
        "        return input_ids, attention_mask"
      ],
      "metadata": {
        "id": "ZK0XosPTBQrV"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# Hyperparameters and settings\n",
        "# ------------------------------\n",
        "MODEL_NAME = \"gpt2\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH_SIZE = 4\n",
        "LR = 6e-5\n",
        "MAX_SEQ_LENGTH = 128      # maximum sequence length for training examples\n",
        "NUM_EPOCHS = 50           # For demonstration we use few epochs (use more in practice)\n",
        "SHOW_SAMPLE_OUTPUTS = True   # Whether to show sample text generations for comparison\n",
        "\n",
        "print(f\"Using device: {DEVICE}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paSFZZjO45T0",
        "outputId": "d7e3be8c-3bba-4aec-b56f-481f73bb24e4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# Load the baseline (reference) GPT-2 model and tokenizer.\n",
        "# This model is used only for generating target logits.\n",
        "# ------------------------------\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "# Ensure the tokenizer has a pad token (set to the EOS token if missing)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "LiUuasNM43rp"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the wikitext-2 dataset using the Hugging Face datasets library.\n",
        "from datasets import load_dataset\n",
        "wikitext_data = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
        "# For demonstration, we take a small subset of the training split.\n",
        "train_texts = [txt for txt in wikitext_data[\"train\"][\"text\"] if len(txt) > 50][:1000]\n",
        "\n",
        "# Create our dataset and dataloader.\n",
        "dataset = WikiTextDataset(tokenizer, train_texts, max_length=MAX_SEQ_LENGTH)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "print(\"Dataset and dataloader ready for training!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MVl18C_BzRs",
        "outputId": "d90112dd-7a91-4786-b1c5-c5b2d3c0671e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset and dataloader ready for training!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# KL-Divergence Loss Function\n",
        "# ------------------------------\n",
        "def kl_divergence_loss(logits_custom, logits_ref, mask):\n",
        "    \"\"\"\n",
        "    Compute a token-wise KL-divergence between the output distributions of the custom model\n",
        "    and the reference model. The loss is averaged over the active (non-padded) tokens.\n",
        "\n",
        "    logits_custom: (B, L, V)\n",
        "    logits_ref:    (B, L, V)\n",
        "    mask:          (B, L) with 1 for active tokens and 0 for padding.\n",
        "    \"\"\"\n",
        "    log_probs_custom = F.log_softmax(logits_custom, dim=-1)\n",
        "    # Detach the reference probabilities to avoid backprop into the reference model.\n",
        "    probs_ref = F.softmax(logits_ref.detach(), dim=-1)\n",
        "    # Compute the per-token KL divergence.\n",
        "    kl = (probs_ref * (probs_ref.log() - log_probs_custom)).sum(-1)  # shape: (B, L)\n",
        "    # Average the loss over the active tokens.\n",
        "    active_tokens = mask.sum()\n",
        "    return (kl * mask).sum() / (active_tokens + 1e-8)"
      ],
      "metadata": {
        "id": "ZNC_NPBOBzf-"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# Initialize the Custom Model\n",
        "# ------------------------------\n",
        "print(\"Initializing custom model with learnable attention masks...\")\n",
        "# We re-load the reference model so that the custom model can copy its embeddings and head.\n",
        "reference_model = GPT2LMHeadModel.from_pretrained(MODEL_NAME).to(DEVICE)\n",
        "reference_model.eval()\n",
        "print(f\"Reference model '{MODEL_NAME}' loaded successfully!\")\n",
        "\n",
        "custom_config = GPT2Config.from_pretrained(MODEL_NAME)\n",
        "custom_model = CustomGPT2LM(custom_config, reference_model).to(DEVICE)\n",
        "\n",
        "# Freeze parameters that are not part of the custom attention (or other parts we want fixed).\n",
        "for name, param in custom_model.named_parameters():\n",
        "    # Here we unfreeze only the parameters that include \"attn\" in their name.\n",
        "    if \"attn\" in name:\n",
        "        param.requires_grad_(True)\n",
        "    else:\n",
        "        param.requires_grad_(False)\n",
        "\n",
        "# Set the custom model to train mode.\n",
        "custom_model.train()\n",
        "\n",
        "# Create the optimizer to update only parameters that require gradients.\n",
        "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, custom_model.parameters()), lr=LR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIAlKHs-BztT",
        "outputId": "793bf1ed-e0ee-41b9-ae2e-391613e9f8f3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing custom model with learnable attention masks...\n",
            "Reference model 'gpt2' loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# Training Loop\n",
        "# ------------------------------\n",
        "print(\"Starting training loop...\")\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    total_loss = 0.0\n",
        "    for input_ids, attn_mask in dataloader:\n",
        "        input_ids, attn_mask = input_ids.to(DEVICE), attn_mask.to(DEVICE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            ref_out = reference_model(input_ids=input_ids, attention_mask=attn_mask)\n",
        "        ref_logits = ref_out.logits\n",
        "\n",
        "        out_custom = custom_model(input_ids=input_ids, attention_mask=attn_mask)\n",
        "        custom_logits = out_custom.logits\n",
        "\n",
        "        loss = kl_divergence_loss(custom_logits, ref_logits, attn_mask)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1} | Avg KL Loss: {total_loss / len(dataloader):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAKgNZOWBz6k",
        "outputId": "f026c548-2d7d-4b2c-d3f6-8aed876725dc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training loop...\n",
            "Epoch 1 | Avg KL Loss: 3.1287\n",
            "Epoch 2 | Avg KL Loss: 2.9838\n",
            "Epoch 3 | Avg KL Loss: 2.8739\n",
            "Epoch 4 | Avg KL Loss: 2.7766\n",
            "Epoch 5 | Avg KL Loss: 2.7217\n",
            "Epoch 6 | Avg KL Loss: 2.6723\n",
            "Epoch 7 | Avg KL Loss: 2.6355\n",
            "Epoch 8 | Avg KL Loss: 2.6146\n",
            "Epoch 9 | Avg KL Loss: 2.5496\n",
            "Epoch 10 | Avg KL Loss: 2.5313\n",
            "Epoch 11 | Avg KL Loss: 2.5233\n",
            "Epoch 12 | Avg KL Loss: 2.5008\n",
            "Epoch 13 | Avg KL Loss: 2.5190\n",
            "Epoch 14 | Avg KL Loss: 2.4935\n",
            "Epoch 15 | Avg KL Loss: 2.4824\n",
            "Epoch 16 | Avg KL Loss: 2.4537\n",
            "Epoch 17 | Avg KL Loss: 2.4543\n",
            "Epoch 18 | Avg KL Loss: 2.4342\n",
            "Epoch 19 | Avg KL Loss: 2.4141\n",
            "Epoch 20 | Avg KL Loss: 2.4021\n",
            "Epoch 21 | Avg KL Loss: 2.4035\n",
            "Epoch 22 | Avg KL Loss: 2.3994\n",
            "Epoch 23 | Avg KL Loss: 2.3822\n",
            "Epoch 24 | Avg KL Loss: 2.3959\n",
            "Epoch 25 | Avg KL Loss: 2.3727\n",
            "Epoch 26 | Avg KL Loss: 2.3844\n",
            "Epoch 27 | Avg KL Loss: 2.3513\n",
            "Epoch 28 | Avg KL Loss: 2.3367\n",
            "Epoch 29 | Avg KL Loss: 2.3392\n",
            "Epoch 30 | Avg KL Loss: 2.3456\n",
            "Epoch 31 | Avg KL Loss: 2.3425\n",
            "Epoch 32 | Avg KL Loss: 2.3603\n",
            "Epoch 33 | Avg KL Loss: 2.3539\n",
            "Epoch 34 | Avg KL Loss: 2.3306\n",
            "Epoch 35 | Avg KL Loss: 2.3262\n",
            "Epoch 36 | Avg KL Loss: 2.3006\n",
            "Epoch 37 | Avg KL Loss: 2.3307\n",
            "Epoch 38 | Avg KL Loss: 2.3162\n",
            "Epoch 39 | Avg KL Loss: 2.3083\n",
            "Epoch 40 | Avg KL Loss: 2.2982\n",
            "Epoch 41 | Avg KL Loss: 2.3132\n",
            "Epoch 42 | Avg KL Loss: 2.3154\n",
            "Epoch 43 | Avg KL Loss: 2.3067\n",
            "Epoch 44 | Avg KL Loss: 2.2964\n",
            "Epoch 45 | Avg KL Loss: 2.3187\n",
            "Epoch 46 | Avg KL Loss: 2.3018\n",
            "Epoch 47 | Avg KL Loss: 2.2787\n",
            "Epoch 48 | Avg KL Loss: 2.2688\n",
            "Epoch 49 | Avg KL Loss: 2.2956\n",
            "Epoch 50 | Avg KL Loss: 2.2994\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# Text Generation Comparison\n",
        "# ------------------------------\n",
        "def generate_text(model, prompt, temperature=0.7, top_k=50, max_length=50):\n",
        "    \"\"\"\n",
        "    Generate text using the provided model and prompt.\n",
        "    Note: We disable caching (use_cache=False) for our custom model.\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
        "    outputs = model.generate(\n",
        "        inputs.input_ids,\n",
        "        max_length=max_length,\n",
        "        do_sample=True,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        repetition_penalty=1.2,\n",
        "        no_repeat_ngram_size=2,\n",
        "        use_cache=False  # Disable caching for compatibility with custom attention.\n",
        "    )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# if SHOW_SAMPLE_OUTPUTS:\n",
        "sample_prompts = [\n",
        "    \"Hello, my name is\",\n",
        "    \"The meaning of life is\",\n",
        "    \"In a shocking turn of events,\",\n",
        "    \"The future of artificial intelligence\"\n",
        "]\n",
        "\n",
        "longer_sample_prompts = [\n",
        "    \"As the sun set behind the towering mountains, the weary traveler finally caught sight of the distant village, its warm lights flickering like tiny stars\",\n",
        "    \"In the year 2157, humanity had finally perfected interstellar travel. The first colony ship, brimming with hope and thousands of eager settlers\",\n",
        "    \"The scientist stared at the glowing vial on the laboratory table, her fingers trembling with anticipation. After years of relentless experimentation\",\n",
        "    \"The detective pushed open the heavy oak door, stepping into a room thick with the scent of old books and something more sinister—fear\"\n",
        "]\n",
        "\n",
        "for prompt in sample_prompts:\n",
        "    ref_text = generate_text(reference_model, prompt, temperature=0.8)\n",
        "    custom_text = generate_text(custom_model, prompt, temperature=0.8)\n",
        "    print(f\"\\nPrompt: {prompt}\")\n",
        "    print(f\"Reference: {ref_text}\")\n",
        "    print(f\"Custom:    {custom_text}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "for prompt in longer_sample_prompts:\n",
        "    ref_text = generate_text(reference_model, prompt, temperature=0.8)\n",
        "    custom_text = generate_text(custom_model, prompt, temperature=0.8)\n",
        "    print(f\"\\nPrompt: {prompt}\")\n",
        "    print(f\"Reference: {ref_text}\")\n",
        "    print(f\"Custom:    {custom_text}\")\n",
        "    print(\"-\" * 80)"
      ],
      "metadata": {
        "id": "noY79OpIBNKF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "880a9477-5ee8-4327-e014-a859abb1785d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prompt: Hello, my name is\n",
            "Reference: Hello, my name is Tom and I'm a high school student. If you're unfamiliar with me then please take note of what's happening to us,\" he said by telephone from his home in Ohio early last month after the class was abruptly canceled for\n",
            "Custom:    Hello, my name is \" when the other major of a lot if she did not so he asked about it's father would be seen as his own. They're actually being shown in an incident with those people was still can't give me to bring\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Prompt: The meaning of life is\n",
            "Reference: The meaning of life is that God has created you to live according as he desires. (Deut 4:15.)\n",
            "This means not only about living in a world without sin or death, but also with the right kind and loving nature we seek\n",
            "Custom:    The meaning of life is also the use to be, where you can find out. The reason for an individual human beings and one who are a major problems with no other than when we're already in some people .\n",
            "In both men's desire becomes\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Prompt: In a shocking turn of events,\n",
            "Reference: In a shocking turn of events, on 2 August last year he was arrested for the murder of two people in his flat. The victims were aged 21 and 22; both men are now dead\n",
            "/Picture: PA / Getty Images A woman who is\n",
            "Custom:    In a shocking turn of events, and to get drunk as well with little girls in the night before entering its relationship between friends who became an investigation. However much like their past\n",
            "\" from this time when he could not allowed for him it is really\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Prompt: The future of artificial intelligence\n",
            "Reference: The future of artificial intelligence.\n",
            "\"We're very happy, and the next big step is to be able that in a few months,\" he said. \"When we do something like this it will help us better understand what our AI systems are capable\n",
            "Custom:    The future of artificial intelligence service, in the first instance; that both sides because there is a particular to be used for those who could not only one's power. Some are always have been very well-ease and his ability , it was at\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Prompt: As the sun set behind the towering mountains, the weary traveler finally caught sight of the distant village, its warm lights flickering like tiny stars\n",
            "Reference: As the sun set behind the towering mountains, the weary traveler finally caught sight of the distant village, its warm lights flickering like tiny stars. He was alone but in darkness for a moment before he heard his brother's cries and saw him pass by it\n",
            "Custom:    As the sun set behind the towering mountains, the weary traveler finally caught sight of the distant village, its warm lights flickering like tiny stars around a temple. The second floor which is still standing right and that has an ancient Egyptian tomb , so it was\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Prompt: In the year 2157, humanity had finally perfected interstellar travel. The first colony ship, brimming with hope and thousands of eager settlers\n",
            "Reference: In the year 2157, humanity had finally perfected interstellar travel. The first colony ship, brimming with hope and thousands of eager settlers in what would become known as \"The New Frontier,\" arrived on a small island between worlds called Galenia IV\n",
            "Custom:    In the year 2157, humanity had finally perfected interstellar travel. The first colony ship, brimming with hope and thousands of eager settlers to return later in a short-day was never before he is not for their way out there were allowed by him\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Prompt: The scientist stared at the glowing vial on the laboratory table, her fingers trembling with anticipation. After years of relentless experimentation\n",
            "Reference: The scientist stared at the glowing vial on the laboratory table, her fingers trembling with anticipation. After years of relentless experimentation and studying various types of food plants she realized that this was a new type of plant – no doubt used for something else! She\n",
            "Custom:    The scientist stared at the glowing vial on the laboratory table, her fingers trembling with anticipation. After years of relentless experimentation . The other members as well-cahin's name , but one could not a long lines for women were also appears in\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Prompt: The detective pushed open the heavy oak door, stepping into a room thick with the scent of old books and something more sinister—fear\n",
            "Reference: The detective pushed open the heavy oak door, stepping into a room thick with the scent of old books and something more sinister—fear. \"What is this?\" I asked softly as we entered,\"Logan\", but he looked over at me.\"\n",
            "Custom:    The detective pushed open the heavy oak door, stepping into a room thick with the scent of old books and something more sinister—fear . The wall. While holding their hands in an ancient monuments are seen when his body shows that was left to use\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}