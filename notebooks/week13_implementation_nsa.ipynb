{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THnHILJXB5t5"
      },
      "source": [
        "# Jupyter Notebook: Custom Attention Optimization\n",
        "\n",
        "Description:\n",
        "------------\n",
        "In this notebook, we will:\n",
        "1. Load a pre-built language model (LLM).\n",
        "2. Create a copy of the model architecture but replace its attention mechanism with a simplified one that only attends to the last 5 tokens (instead of all previous tokens).\n",
        "3. Implement a process to compare the outputs of both models and compute a KL-divergence loss.\n",
        "4. Optimize the custom model's parameters by minimizing the KL-divergence between the two modelsâ€™ distributions.\n",
        "5. Demonstrate how to evaluate and compare both models on sample data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOZg0M9K85VU",
        "outputId": "786f94d0-0fc8-4ff0-e1ab-f8d406862226"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrmsZrrOk2z7",
        "outputId": "18067bb1-d233-404d-a5ea-092c4e80d5d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Collecting native-sparse-attention-pytorch\n",
            "  Downloading native_sparse_attention_pytorch-0.2.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: einops>=0.8.1 in /usr/local/lib/python3.11/dist-packages (from native-sparse-attention-pytorch) (0.8.1)\n",
            "Collecting einx>=0.3.0 (from native-sparse-attention-pytorch)\n",
            "  Downloading einx-0.3.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting jaxtyping (from native-sparse-attention-pytorch)\n",
            "  Downloading jaxtyping-0.3.1-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting local-attention>=1.11.1 (from native-sparse-attention-pytorch)\n",
            "  Downloading local_attention-1.11.1-py3-none-any.whl.metadata (907 bytes)\n",
            "Collecting rotary-embedding-torch (from native-sparse-attention-pytorch)\n",
            "  Downloading rotary_embedding_torch-0.8.6-py3-none-any.whl.metadata (675 bytes)\n",
            "Requirement already satisfied: torch>=2.5 in /usr/local/lib/python3.11/dist-packages (from native-sparse-attention-pytorch) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from einx>=0.3.0->native-sparse-attention-pytorch) (2.0.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from einx>=0.3.0->native-sparse-attention-pytorch) (1.13.1)\n",
            "Requirement already satisfied: frozendict in /usr/local/lib/python3.11/dist-packages (from einx>=0.3.0->native-sparse-attention-pytorch) (2.4.6)\n",
            "Collecting hyper-connections>=0.1.8 (from local-attention>=1.11.1->native-sparse-attention-pytorch)\n",
            "  Downloading hyper_connections-0.1.15-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.5->native-sparse-attention-pytorch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5->native-sparse-attention-pytorch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.5->native-sparse-attention-pytorch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5->native-sparse-attention-pytorch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.5->native-sparse-attention-pytorch) (2024.12.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.5->native-sparse-attention-pytorch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.5->native-sparse-attention-pytorch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.5->native-sparse-attention-pytorch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.5->native-sparse-attention-pytorch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.5->native-sparse-attention-pytorch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.5->native-sparse-attention-pytorch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.5->native-sparse-attention-pytorch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.5->native-sparse-attention-pytorch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.5->native-sparse-attention-pytorch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5->native-sparse-attention-pytorch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5->native-sparse-attention-pytorch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5->native-sparse-attention-pytorch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.5->native-sparse-attention-pytorch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5->native-sparse-attention-pytorch) (3.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->einx>=0.3.0->native-sparse-attention-pytorch) (1.3.0)\n",
            "Collecting wadler-lindig>=0.1.3 (from jaxtyping->native-sparse-attention-pytorch)\n",
            "  Downloading wadler_lindig-0.1.5-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.5->native-sparse-attention-pytorch) (3.0.2)\n",
            "Downloading native_sparse_attention_pytorch-0.2.0-py3-none-any.whl (27 kB)\n",
            "Downloading einx-0.3.0-py3-none-any.whl (102 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m103.0/103.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading local_attention-1.11.1-py3-none-any.whl (9.4 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m125.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m97.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxtyping-0.3.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55.3/55.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rotary_embedding_torch-0.8.6-py3-none-any.whl (5.6 kB)\n",
            "Downloading hyper_connections-0.1.15-py3-none-any.whl (15 kB)\n",
            "Downloading wadler_lindig-0.1.5-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: wadler-lindig, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jaxtyping, einx, nvidia-cusolver-cu12, rotary-embedding-torch, hyper-connections, local-attention, native-sparse-attention-pytorch\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed einx-0.3.0 hyper-connections-0.1.15 jaxtyping-0.3.1 local-attention-1.11.1 native-sparse-attention-pytorch-0.2.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 rotary-embedding-torch-0.8.6 wadler-lindig-0.1.5\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install native-sparse-attention-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############## # Code Block 1: Imports & Config ##############\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "from native_sparse_attention_pytorch import SparseAttention\n",
        "import time\n",
        "import psutil\n",
        "import os\n",
        "\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "MODEL_NAME = \"gpt2\"\n",
        "SEQ_LEN = 128\n",
        "BATCH_SIZE = 4\n",
        "NUM_HEADS = 4\n",
        "COMPRESS_RATIO = 0.25\n",
        "WINDOW_SIZE = 64\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "# Native sparse attention configuration\n",
        "SPARSE_CONFIG = {\n",
        "    \"dim\": None,  # Will be set in the model\n",
        "    \"dim_head\": 64,  # Dimension per head\n",
        "    \"heads\": NUM_HEADS,\n",
        "    \"sliding_window_size\": 2,  # Local attention window\n",
        "    \"compress_block_size\": 4,  # Size of blocks to compress\n",
        "    \"selection_block_size\": 4,  # Size of blocks to select from\n",
        "    \"num_selected_blocks\": 2,  # Number of blocks to select\n",
        "}"
      ],
      "metadata": {
        "id": "_b9XN6_T5T-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############## # Code Block 2: Sparse Attention Components ##############\n",
        "class CompressedGlobalAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, compress_ratio):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.compress_ratio = compress_ratio\n",
        "\n",
        "        self.Wq = nn.Linear(embed_dim, embed_dim)\n",
        "        self.Wk = nn.Linear(embed_dim, embed_dim)\n",
        "        self.Wv = nn.Linear(embed_dim, embed_dim)\n",
        "        self.compression = nn.Linear(embed_dim, 1)\n",
        "        self.expansion = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x, attention_mask=None):\n",
        "        B, T, D = x.shape\n",
        "        keep_num = max(1, int(T * self.compress_ratio))\n",
        "\n",
        "        # Token compression\n",
        "        importance = self.compression(x).squeeze(-1)\n",
        "        _, keep_idx = torch.topk(importance, k=keep_num, dim=-1)\n",
        "        x_compressed = torch.gather(x, 1, keep_idx.unsqueeze(-1).expand(-1, -1, D))\n",
        "\n",
        "        # Projections\n",
        "        Q = self.Wq(x).view(B, T, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = (\n",
        "            self.Wk(x_compressed)\n",
        "            .view(B, keep_num, self.num_heads, self.head_dim)\n",
        "            .permute(0, 2, 1, 3)\n",
        "        )\n",
        "        V = (\n",
        "            self.Wv(x_compressed)\n",
        "            .view(B, keep_num, self.num_heads, self.head_dim)\n",
        "            .permute(0, 2, 1, 3)\n",
        "        )\n",
        "\n",
        "        # Attention\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-1, -2)) / math.sqrt(self.head_dim)\n",
        "\n",
        "        # Masking\n",
        "        if attention_mask is not None:\n",
        "            compressed_mask = torch.gather(attention_mask, 1, keep_idx)\n",
        "            attn_scores = attn_scores.masked_fill(\n",
        "                compressed_mask.unsqueeze(1).unsqueeze(2) == 0, -1e10\n",
        "            )\n",
        "\n",
        "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
        "        output = torch.matmul(attn_weights, V)\n",
        "        output = output.permute(0, 2, 1, 3).contiguous().view(B, T, D)\n",
        "\n",
        "        output = self.expansion(output)  # Ensure output has correct embedding dimension\n",
        "        output = output[:, : x.size(1), :]\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class LocalWindowAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, window_size):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.window_size = window_size\n",
        "\n",
        "        self.Wq = nn.Linear(embed_dim, embed_dim)\n",
        "        self.Wk = nn.Linear(embed_dim, embed_dim)\n",
        "        self.Wv = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def create_window_mask(self, seq_len, device):\n",
        "        mask = torch.zeros(seq_len, seq_len, device=device)\n",
        "        for i in range(seq_len):\n",
        "            start = max(0, i - self.window_size // 2)\n",
        "            end = min(seq_len, i + self.window_size // 2 + 1)\n",
        "            mask[i, start:end] = 1\n",
        "        return mask.unsqueeze(0).unsqueeze(1)\n",
        "\n",
        "    def forward(self, x, attention_mask=None):\n",
        "        B, T, D = x.shape\n",
        "        window_mask = self.create_window_mask(T, x.device)\n",
        "\n",
        "        Q = self.Wq(x).view(B, T, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = self.Wk(x).view(B, T, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = self.Wv(x).view(B, T, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-1, -2)) / math.sqrt(self.head_dim)\n",
        "        attn_scores = attn_scores.masked_fill(window_mask == 0, -1e10)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(\n",
        "                attention_mask.unsqueeze(1).unsqueeze(2) == 0, -1e10\n",
        "            )\n",
        "\n",
        "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
        "        output = torch.matmul(attn_weights, V)\n",
        "        return output.permute(0, 2, 1, 3).contiguous().view(B, T, D)\n",
        "\n",
        "\n",
        "class HierarchicalSparseAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, window_size, compress_ratio):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads  # ğŸ”¹ Store num_heads\n",
        "        self.local_attn = LocalWindowAttention(embed_dim, num_heads, window_size)\n",
        "        self.global_attn = CompressedGlobalAttention(\n",
        "            embed_dim, num_heads, compress_ratio\n",
        "        )\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.Linear(\n",
        "                embed_dim, num_heads * 2\n",
        "            ),  # Ensure output is [batch, seq_len, num_heads * 2]\n",
        "            nn.Softmax(dim=-1),\n",
        "        )\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x, attention_mask=None):\n",
        "        # Get outputs from local and global attention modules.\n",
        "        local_out = self.local_attn(x, attention_mask)  # Expected shape: (B, T, D)\n",
        "        global_out = self.global_attn(x, attention_mask)  # Expected shape: (B, T, D)\n",
        "\n",
        "        B, T, D = x.size()\n",
        "        head_dim = D // self.num_heads  # Ensure D is divisible by num_heads\n",
        "\n",
        "        # Compute gating weights.\n",
        "        # self.gate should output a tensor of shape (B, T, num_heads*2)\n",
        "        gate_out = self.gate(x)  # Shape: (B, T, num_heads*2)\n",
        "        # Reshape to (B, T, num_heads, 2) where last dim holds [local_gate, global_gate]\n",
        "        gates = gate_out.view(B, T, self.num_heads, 2)\n",
        "        # Unbind the last dimension into two tensors\n",
        "        local_gate = gates[..., 0]  # Shape: (B, T, num_heads)\n",
        "        global_gate = gates[..., 1]  # Shape: (B, T, num_heads)\n",
        "\n",
        "        # Reshape attention outputs to split heads: (B, T, num_heads, head_dim)\n",
        "        local_out_heads = local_out.view(B, T, self.num_heads, head_dim)\n",
        "        global_out_heads = global_out.view(B, T, self.num_heads, head_dim)\n",
        "\n",
        "        # Ensure the gate tensors have an extra dimension for broadcasting: (B, T, num_heads, 1)\n",
        "        local_gate = local_gate.unsqueeze(-1)\n",
        "        global_gate = global_gate.unsqueeze(-1)\n",
        "\n",
        "        # Element-wise multiply each head output by its corresponding gate weight\n",
        "        combined = local_out_heads * local_gate + global_out_heads * global_gate\n",
        "        # Reshape back to (B, T, D)\n",
        "        combined = combined.view(B, T, D)\n",
        "        return self.out_proj(combined)"
      ],
      "metadata": {
        "id": "5swz0wzQ5RNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############## # Code Block 3: Custom GPT-2 Model ##############\n",
        "class SparseGPT2(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.wte = nn.Embedding(config.vocab_size, config.hidden_size)\n",
        "        self.wpe = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "        self.drop = nn.Dropout(config.embd_pdrop)\n",
        "\n",
        "        # Create native sparse attention layer with correct parameters\n",
        "        sparse_config = SPARSE_CONFIG.copy()\n",
        "        sparse_config[\"dim\"] = config.hidden_size\n",
        "        sparse_config[\"compress_block_sliding_stride\"] = 2\n",
        "        self.sparse_attn = SparseAttention(**sparse_config)\n",
        "\n",
        "        self.h = nn.ModuleList(\n",
        "            [\n",
        "                nn.ModuleDict(\n",
        "                    {\n",
        "                        \"attn\": self.sparse_attn,\n",
        "                        \"ln_1\": nn.LayerNorm(config.hidden_size),\n",
        "                        \"mlp\": nn.Sequential(\n",
        "                            nn.Linear(config.hidden_size, 4 * config.hidden_size),\n",
        "                            nn.GELU(),\n",
        "                            nn.Linear(4 * config.hidden_size, config.hidden_size),\n",
        "                        ),\n",
        "                        \"ln_2\": nn.LayerNorm(config.hidden_size),\n",
        "                    }\n",
        "                )\n",
        "                for _ in range(config.num_hidden_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.ln_f = nn.LayerNorm(config.hidden_size)\n",
        "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        B, T = input_ids.size()\n",
        "        pos_ids = torch.arange(T, device=DEVICE).unsqueeze(0)\n",
        "\n",
        "        x = self.drop(self.wte(input_ids) + self.wpe(pos_ids))\n",
        "\n",
        "        attn_out = torch.zeros_like(x)\n",
        "\n",
        "        for block in self.h:\n",
        "            # Apply layer norm before attention\n",
        "            normed_x = block[\"ln_1\"](x)\n",
        "            attention_result = block[\"attn\"](normed_x)\n",
        "\n",
        "            # Apply sparse attention and handle tuple output\n",
        "            # attn_output = block[\"attn\"](normed_x)\n",
        "            if attention_result is not None:\n",
        "                attn_out = attention_result\n",
        "            # # If attn_output is a tuple, take the first element (the main output)\n",
        "            # if isinstance(attn_output, tuple):\n",
        "            #     attn_out = attn_output[0]\n",
        "            # else:\n",
        "            #     attn_out = attn_output\n",
        "\n",
        "            # Apply mask after attention if provided\n",
        "            if attention_mask is not None:\n",
        "                attn_out = attn_out * attention_mask.unsqueeze(-1)\n",
        "\n",
        "            x = x + attn_out\n",
        "            x = x + block[\"mlp\"](block[\"ln_2\"](x))\n",
        "\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "        return logits  # Return only the logits, not a tuple\n",
        "\n",
        "\n",
        "############## # Code Block 4: Training Setup ##############\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Reference model\n",
        "ref_model = GPT2LMHeadModel.from_pretrained(MODEL_NAME).to(DEVICE)\n",
        "ref_model.eval()\n",
        "\n",
        "# Custom model with native sparse attention\n",
        "cust_config = GPT2Config.from_pretrained(MODEL_NAME)\n",
        "cust_model = SparseGPT2(cust_config).to(DEVICE)\n",
        "\n",
        "# Initialize with pretrained weights\n",
        "pretrained_state_dict = ref_model.state_dict()\n",
        "cust_model.load_state_dict(pretrained_state_dict, strict=False)\n",
        "\n",
        "# Use a lower learning rate for fine-tuning\n",
        "optimizer = torch.optim.AdamW(cust_model.parameters(), lr=1e-3)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n"
      ],
      "metadata": {
        "id": "VU1BGXh95MO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############## # Code Block 5: Training Loop ##############\n",
        "class UsageTracker:\n",
        "    def __init__(self):\n",
        "        self.start_time = None\n",
        "        self.start_cpu_time = None\n",
        "        self.train_stats = {\n",
        "            \"epoch\": [], \"step\": [],\n",
        "            \"wall_time_s\": [], \"cpu_time_s\": [],\n",
        "            \"cpu_mem_mb\": [], \"gpu_alloc_mb\": [], \"gpu_reserved_mb\": []\n",
        "        }\n",
        "        self.infer_stats = {\n",
        "            \"max_token_length\": [],\n",
        "            \"wall_time_s\": [], \"cpu_time_s\": [],\n",
        "            \"cpu_mem_mb\": [], \"gpu_alloc_mb\": [], \"gpu_reserved_mb\": []\n",
        "        }\n",
        "\n",
        "    def start_tracking(self):\n",
        "        self.start_time = time.time()\n",
        "        self.start_cpu_time = time.process_time()\n",
        "\n",
        "    def stop_tracking(self, is_training, epoch=None, step=None, max_token_length=None):\n",
        "        if self.start_time is None or self.start_cpu_time is None:\n",
        "            raise ValueError(\"Tracking not started. Call start_tracking() first.\")\n",
        "\n",
        "        end_time = time.time()\n",
        "        end_cpu_time = time.process_time()\n",
        "\n",
        "        proc = psutil.Process(os.getpid())\n",
        "        cpu_mem = proc.memory_info().rss / (1024**2)\n",
        "        if torch.cuda.is_available():\n",
        "            gpu_alloc    = torch.cuda.memory_allocated(DEVICE) / (1024**2)\n",
        "            gpu_reserved = torch.cuda.memory_reserved(DEVICE) / (1024**2)\n",
        "        else:\n",
        "            gpu_alloc = gpu_reserved = 0.0\n",
        "\n",
        "        obj = self.train_stats if is_training else self.infer_stats\n",
        "        obj[\"wall_time_s\"].append(end_time - self.start_time)\n",
        "        obj[\"cpu_time_s\"].append(end_cpu_time - self.start_cpu_time)\n",
        "        obj[\"cpu_mem_mb\"].append(cpu_mem)\n",
        "        obj[\"gpu_alloc_mb\"].append(gpu_alloc)\n",
        "        obj[\"gpu_reserved_mb\"].append(gpu_reserved)\n",
        "        if is_training:\n",
        "            obj[\"epoch\"].append(epoch + 1)\n",
        "            obj[\"step\"].append(step + 1)\n",
        "        else:\n",
        "            obj[\"max_token_length\"].append(max_token_length)\n",
        "\n",
        "        self.start_time = None\n",
        "        self.start_cpu_time = None\n",
        "\n",
        "    def plot(self):\n",
        "        df_train = pd.DataFrame(self.train_stats)\n",
        "        df_inf   = pd.DataFrame(self.infer_stats)\n",
        "\n",
        "        # --- Training plots (line plots vs. step) ---\n",
        "        for metric in [\"wall_time_s\", \"cpu_time_s\", \"cpu_mem_mb\", \"gpu_alloc_mb\", \"gpu_reserved_mb\"]:\n",
        "            plt.figure(figsize=(8, 4))\n",
        "            plt.plot(df_train[\"step\"], df_train[metric])\n",
        "            plt.xlabel(\"Training Step\")\n",
        "            plt.ylabel(metric.replace(\"_\", \" \").title())\n",
        "            plt.title(f\"{metric.replace('_', ' ').title()} During Training\")\n",
        "            plt.show()\n",
        "\n",
        "        # # --- Inference plots (bar plots vs. call index) ---\n",
        "        # for metric in [\"wall_time_s\", \"cpu_time_s\", \"cpu_mem_mb\", \"gpu_alloc_mb\", \"gpu_reserved_mb\"]:\n",
        "        #     plt.figure(figsize=(8, 4))\n",
        "        #     plt.bar(range(len(df_inf)), df_inf[metric])\n",
        "        #     plt.xlabel(\"Inference Call #\")\n",
        "        #     plt.ylabel(metric.replace(\"_\", \" \").title())\n",
        "        #     plt.title(f\"{metric.replace('_', ' ').title()} Per Inference\")\n",
        "        #     plt.show()\n",
        "\n",
        "        prompt_lengths = self.infer_stats[\"max_token_length\"]\n",
        "\n",
        "        # --- Inference plots (bar plots vs. token length) ---\n",
        "        for metric in [\"wall_time_s\", \"cpu_time_s\", \"cpu_mem_mb\", \"gpu_alloc_mb\", \"gpu_reserved_mb\"]:\n",
        "            plt.figure(figsize=(8, 4))\n",
        "            plt.scatter(prompt_lengths, self.infer_stats[metric])\n",
        "            plt.xlabel(\"Prompt Token Length\")\n",
        "            plt.ylabel(metric.replace(\"_\", \" \").title())\n",
        "            plt.title(f\"{metric.replace('_', ' ').title()} vs. Prompt Length\")\n",
        "            plt.show()\n",
        "\n",
        "def kl_divergence_loss(logits_custom, logits_ref, mask, temperature):\n",
        "    # 1) soften both distributions by T\n",
        "    logp_c = F.log_softmax(logits_custom / temperature, dim=-1)\n",
        "    p_r   = F.softmax(   logits_ref.detach() / temperature, dim=-1)\n",
        "\n",
        "    # 2) perâ€‘token KL\n",
        "    kl = (p_r * (p_r.log() - logp_c)).sum(-1)  # (B, L)\n",
        "\n",
        "    # 3) average over real tokens and reâ€‘scale by T^2\n",
        "    return (kl * mask).sum() / mask.sum() * (temperature * temperature)\n",
        "\n",
        "def train_step(batch, epoch, step):\n",
        "    tracker.start_tracking()\n",
        "\n",
        "    inputs = batch.to(DEVICE)\n",
        "    attention_mask = (inputs != tokenizer.pad_token_id).float()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        ref_outputs = ref_model(inputs, attention_mask=attention_mask)\n",
        "        if isinstance(ref_outputs, tuple):\n",
        "            ref_logits = ref_outputs[0]\n",
        "        else:\n",
        "            ref_logits = ref_outputs.logits  # Extract logits from the output object\n",
        "\n",
        "    cust_outputs = cust_model(inputs, attention_mask=attention_mask)\n",
        "    if isinstance(cust_outputs, tuple):\n",
        "        cust_logits = cust_outputs[0]\n",
        "    else:\n",
        "        cust_logits = cust_outputs\n",
        "\n",
        "    # Use KL divergence loss with temperature\n",
        "    temperature = 0.7\n",
        "    loss = F.kl_div(\n",
        "        F.log_softmax(cust_logits / temperature, dim=-1),\n",
        "        F.softmax(ref_logits / temperature, dim=-1).detach(),\n",
        "        reduction=\"sum\"\n",
        "    ) * (temperature**2)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(cust_model.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "\n",
        "    tracker.stop_tracking(is_training=True, epoch=epoch, step=step)\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "def train_epoch(loader, epoch):\n",
        "    cust_model.train()\n",
        "    total_loss = 0\n",
        "    loss_vals = []\n",
        "    for i, batch in tqdm(enumerate(loader), desc=\"Training\"):\n",
        "        loss = train_step(batch, epoch, i)\n",
        "        total_loss += loss\n",
        "        loss_vals.append(loss)\n",
        "    return total_loss / len(loader), loss_vals\n"
      ],
      "metadata": {
        "id": "CDvyVsHc5Iwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############## # Code Block 6: Generation & Evaluation ##############\n",
        "def generate(model, tokenizer, prompt, max_length=50, temperature=0.7, top_k=50, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=False).to(device)\n",
        "    input_ids = input_ids[:1]  # Ensure we only have one batch dimension\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids)\n",
        "            logits = outputs.logits if hasattr(outputs, \"logits\") else outputs\n",
        "\n",
        "            # Get logits for the last token\n",
        "            if logits.dim() == 3:  # Standard shape [batch, seq, vocab]\n",
        "                next_token_logits = logits[:, -1, :] / temperature\n",
        "            elif logits.dim() == 2:  # If somehow we got [batch*seq, vocab]\n",
        "                # We only care about the last token's logits\n",
        "                next_token_logits = logits[-1:, :] / temperature\n",
        "            else:\n",
        "                raise ValueError(f\"Unexpected logits shape: {logits.shape}\")\n",
        "\n",
        "            # Apply top-k filtering\n",
        "            top_k_logits, top_k_indices = torch.topk(next_token_logits, k=top_k, dim=-1)\n",
        "\n",
        "            # Convert to probabilities\n",
        "            probs = F.softmax(top_k_logits, dim=-1)\n",
        "\n",
        "            # Sample next token index from top-k logits\n",
        "            next_token_idx = torch.multinomial(probs, num_samples=1)\n",
        "            next_token = top_k_indices.gather(1, next_token_idx)\n",
        "\n",
        "            # Ensure next_token has shape [1, 1]\n",
        "            next_token = next_token[-1:, :]  # Take only the last row if needed\n",
        "\n",
        "            # Concatenate to input_ids\n",
        "            input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
        "\n",
        "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "GDU-Hj6S5FAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############## # Code Block 7: Dataset Preparation ##############\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "class WikiDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, seq_len):\n",
        "        self.samples = []\n",
        "        for text in texts:\n",
        "            # Tokenize each text separately, without adding special tokens\n",
        "            token_ids = tokenizer.encode(text, add_special_tokens=False)\n",
        "            # Split token_ids into chunks of length seq_len\n",
        "            for i in range(0, len(token_ids), seq_len):\n",
        "                chunk = token_ids[i : i + seq_len]\n",
        "                # Only add full chunks to avoid very short sequences\n",
        "                if len(chunk) == seq_len:\n",
        "                    self.samples.append(torch.tensor(chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx]\n",
        "\n",
        "\n",
        "# Load a small subset (e.g., 1% of the train split) of WikiText data\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train[:100%]\")\n",
        "texts = dataset[\"text\"]\n",
        "wiki_dataset = WikiDataset(texts, tokenizer, SEQ_LEN)\n",
        "\n",
        "# Create a DataLoader for training\n",
        "train_loader = DataLoader(wiki_dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "id": "0JygiSf75Ayu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0d4Q3hFk1ey",
        "outputId": "f02ad99e-a48c-40fb-93b3-bd0dee419d7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 2571it [07:00,  6.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 - Average Loss: 715.3863\n",
            "Saved checkpoint for epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 2571it [07:02,  6.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10 - Average Loss: 481.0767\n",
            "\n",
            "Generating sample text:\n",
            "Reference: Artificial intelligence will be the next big thing, and I think there's a lot of excitement about it. I think we're going to see it for a very long time.\"\n",
            "\n",
            "To hear a full conversation with Google about the future of robotics, click here\n",
            "Custom: Artificial intelligence (a Dienius. Singh was the only one one hour ending up the city, which time the world world world where they it possible possible possible possible possible possible possible any other other other regions and to the mainland, the enemy power that when the\n",
            "\n",
            "\n",
            "Saved checkpoint for epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 2571it [07:02,  6.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10 - Average Loss: 393.5711\n",
            "Saved checkpoint for epoch 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 2571it [07:03,  6.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10 - Average Loss: 333.3250\n",
            "\n",
            "Generating sample text:\n",
            "Reference: Artificial intelligence can also help us understand the world without having to go through a much more complicated process of training.\n",
            "\n",
            "\"We know in this area that we can use artificial intelligence to learn from the human mind,\" says Gershman. \"In our\n",
            "Custom: Artificial intelligence is the reason for a series of different combinations systems:-t. that a higher resolution of that sentence spacing has to a given the form. .\n",
            "XWY , the system has been carried over from other factors. The original design is used\n",
            "\n",
            "\n",
            "Saved checkpoint for epoch 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 2571it [07:01,  6.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10 - Average Loss: 284.8573\n",
            "Saved checkpoint for epoch 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 2571it [07:01,  6.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10 - Average Loss: 243.1686\n",
            "\n",
            "Generating sample text:\n",
            "Reference: Artificial intelligence is a complex subject. But the goal of Artificial Intelligence is to be able to answer questions that all humans would have to ask before we can truly understand the nature of our world.\n",
            "\n",
            "Today, Artificial Intelligence is being used to solve problems like climate\n",
            "Custom: Artificial intelligence and a very good deal with his family and father, have been a member of five children aged and married couples. and children three children , and a family of English children. on on a farm were made of a girl who just played with at the\n",
            "\n",
            "\n",
            "Saved checkpoint for epoch 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 2571it [07:03,  6.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/10 - Average Loss: 207.8062\n",
            "Saved checkpoint for epoch 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 2571it [07:00,  6.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/10 - Average Loss: 179.4301\n",
            "\n",
            "Generating sample text:\n",
            "Reference: Artificial intelligence, known as AI, is a fascinating subject that has become more controversial. The idea of artificial intelligence is so well known to many that it has also been suggested that scientists are looking into the possibility of AI. The issue is more complicated than that.\n",
            "Custom: Artificial intelligence, which is in the midstententency- the debate is an \"fridiazza in the Land of Light Justice (VNG ) ] ] ] ] ] ] ] ] ] shall be a \"matter \" ( [C ), )\n",
            "\n",
            "\n",
            "Saved checkpoint for epoch 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 2571it [07:00,  6.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/10 - Average Loss: 159.2151\n",
            "Saved checkpoint for epoch 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 2571it [07:01,  6.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10 - Average Loss: 147.9481\n",
            "\n",
            "Generating sample text:\n",
            "Reference: Artificial intelligence is a major new field in tech, but the technology is not yet ready for commercial use.\n",
            "\n",
            "The company behind the project, DeepMind, says it has developed a prototype for \"an intelligent machine that can solve real-world problems in a\n",
            "Custom: Artificial intelligence was a \"wah-style \" to describe a man of great authority to ' to the right-ing and the need for good . . I've Gog , his nephew, the \"master , a good guy play-ing him into the\n",
            "\n",
            "\n",
            "Saved checkpoint for epoch 10\n",
            "\n",
            "Final generation comparison:\n",
            "Reference: Artificial intelligence has a history of being used to achieve very particular goals, and we see it often in the search for a solution to a problem.\n",
            "\n",
            "The main problem is not the technology itself - it is the amount of data that can be processed and stored\n",
            "Custom: Artificial intelligence and the other major factors that do not apply the fundamental conclusions, it may be a useful consequence or might be justified to to understand and enforce the needs to to to to be filled with to one another one or a second one-\n",
            "vigial\n"
          ]
        }
      ],
      "source": [
        "############## # Code Block 8: Training Execution ##############\n",
        "tracker = UsageTracker()\n",
        "\n",
        "CHECKPOINT_DIR = '/content/drive/MyDrive/attention_optimization/model_checkpoints'\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "\n",
        "    avg_loss, loss_vals = train_epoch(train_loader, epoch)\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} - Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Generate sample text after each epoch\n",
        "    if (epoch + 1) % 2 == 0:  # Generate every 2 epochs\n",
        "        print(\"\\nGenerating sample text:\")\n",
        "        prompt = \"Artificial intelligence\"\n",
        "        print(\"Reference:\", generate(ref_model, tokenizer, prompt, temperature=0.7, top_k=50))\n",
        "        print(\"Custom:\", generate(cust_model, tokenizer, prompt, temperature=0.7, top_k=50))\n",
        "        print(\"\\n\")\n",
        "\n",
        "    # checkpoint model vars\n",
        "    checkpoint = {\n",
        "        'epoch': epoch + 1, # next epoch to start from\n",
        "        'model_state_dict': cust_model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss_vals': loss_vals,\n",
        "        # 'val_loss_vals': val_loss_vals,\n",
        "        # 'all_alphas': all_alphas\n",
        "    }\n",
        "    torch.save(checkpoint, os.path.join(CHECKPOINT_DIR, f'ckpt_epoch_{epoch+1}.pth'))\n",
        "    print(f\"Saved checkpoint for epoch {epoch+1}\")\n",
        "\n",
        "# Final generation comparison\n",
        "prompt = \"Artificial intelligence\"\n",
        "print(\"\\nFinal generation comparison:\")\n",
        "print(\"Reference:\", generate(ref_model, tokenizer, prompt, temperature=0.7, top_k=50))\n",
        "print(\"Custom:\", generate(cust_model, tokenizer, prompt, temperature=0.7, top_k=50))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############## # Code Block 1: Imports & Config ##############\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "from native_sparse_attention_pytorch import SparseAttention\n",
        "import time\n",
        "import psutil\n",
        "import os\n",
        "\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "MODEL_NAME = \"gpt2\"\n",
        "SEQ_LEN = 128\n",
        "BATCH_SIZE = 4\n",
        "NUM_HEADS = 4\n",
        "COMPRESS_RATIO = 0.25\n",
        "WINDOW_SIZE = 64\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Native sparse attention configuration\n",
        "SPARSE_CONFIG = {\n",
        "    \"dim\": None,  # Will be set in the model\n",
        "    \"dim_head\": 64,  # Dimension per head\n",
        "    \"heads\": NUM_HEADS,\n",
        "    \"sliding_window_size\": 2,  # Local attention window\n",
        "    \"compress_block_size\": 4,  # Size of blocks to compress\n",
        "    \"selection_block_size\": 4,  # Size of blocks to select from\n",
        "    \"num_selected_blocks\": 2,  # Number of blocks to select\n",
        "}"
      ],
      "metadata": {
        "id": "qdb0u1Nn59y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############## # Code Block 2: Sparse Attention Components ##############\n",
        "class CompressedGlobalAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, compress_ratio):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.compress_ratio = compress_ratio\n",
        "\n",
        "        self.Wq = nn.Linear(embed_dim, embed_dim)\n",
        "        self.Wk = nn.Linear(embed_dim, embed_dim)\n",
        "        self.Wv = nn.Linear(embed_dim, embed_dim)\n",
        "        self.compression = nn.Linear(embed_dim, 1)\n",
        "        self.expansion = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x, attention_mask=None):\n",
        "        B, T, D = x.shape\n",
        "        keep_num = max(1, int(T * self.compress_ratio))\n",
        "\n",
        "        # Token compression\n",
        "        importance = self.compression(x).squeeze(-1)\n",
        "        _, keep_idx = torch.topk(importance, k=keep_num, dim=-1)\n",
        "        x_compressed = torch.gather(x, 1, keep_idx.unsqueeze(-1).expand(-1, -1, D))\n",
        "\n",
        "        # Projections\n",
        "        Q = self.Wq(x).view(B, T, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = (\n",
        "            self.Wk(x_compressed)\n",
        "            .view(B, keep_num, self.num_heads, self.head_dim)\n",
        "            .permute(0, 2, 1, 3)\n",
        "        )\n",
        "        V = (\n",
        "            self.Wv(x_compressed)\n",
        "            .view(B, keep_num, self.num_heads, self.head_dim)\n",
        "            .permute(0, 2, 1, 3)\n",
        "        )\n",
        "\n",
        "        # Attention\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-1, -2)) / math.sqrt(self.head_dim)\n",
        "\n",
        "        # Masking\n",
        "        if attention_mask is not None:\n",
        "            compressed_mask = torch.gather(attention_mask, 1, keep_idx)\n",
        "            attn_scores = attn_scores.masked_fill(\n",
        "                compressed_mask.unsqueeze(1).unsqueeze(2) == 0, -1e10\n",
        "            )\n",
        "\n",
        "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
        "        output = torch.matmul(attn_weights, V)\n",
        "        output = output.permute(0, 2, 1, 3).contiguous().view(B, T, D)\n",
        "\n",
        "        output = self.expansion(output)  # Ensure output has correct embedding dimension\n",
        "        output = output[:, : x.size(1), :]\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class LocalWindowAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, window_size):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.window_size = window_size\n",
        "\n",
        "        self.Wq = nn.Linear(embed_dim, embed_dim)\n",
        "        self.Wk = nn.Linear(embed_dim, embed_dim)\n",
        "        self.Wv = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def create_window_mask(self, seq_len, device):\n",
        "        mask = torch.zeros(seq_len, seq_len, device=device)\n",
        "        for i in range(seq_len):\n",
        "            start = max(0, i - self.window_size // 2)\n",
        "            end = min(seq_len, i + self.window_size // 2 + 1)\n",
        "            mask[i, start:end] = 1\n",
        "        return mask.unsqueeze(0).unsqueeze(1)\n",
        "\n",
        "    def forward(self, x, attention_mask=None):\n",
        "        B, T, D = x.shape\n",
        "        window_mask = self.create_window_mask(T, x.device)\n",
        "\n",
        "        Q = self.Wq(x).view(B, T, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = self.Wk(x).view(B, T, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = self.Wv(x).view(B, T, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-1, -2)) / math.sqrt(self.head_dim)\n",
        "        attn_scores = attn_scores.masked_fill(window_mask == 0, -1e10)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(\n",
        "                attention_mask.unsqueeze(1).unsqueeze(2) == 0, -1e10\n",
        "            )\n",
        "\n",
        "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
        "        output = torch.matmul(attn_weights, V)\n",
        "        return output.permute(0, 2, 1, 3).contiguous().view(B, T, D)\n",
        "\n",
        "\n",
        "class HierarchicalSparseAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, window_size, compress_ratio):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads  # ğŸ”¹ Store num_heads\n",
        "        self.local_attn = LocalWindowAttention(embed_dim, num_heads, window_size)\n",
        "        self.global_attn = CompressedGlobalAttention(\n",
        "            embed_dim, num_heads, compress_ratio\n",
        "        )\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.Linear(\n",
        "                embed_dim, num_heads * 2\n",
        "            ),  # Ensure output is [batch, seq_len, num_heads * 2]\n",
        "            nn.Softmax(dim=-1),\n",
        "        )\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x, attention_mask=None):\n",
        "        # Get outputs from local and global attention modules.\n",
        "        local_out = self.local_attn(x, attention_mask)  # Expected shape: (B, T, D)\n",
        "        global_out = self.global_attn(x, attention_mask)  # Expected shape: (B, T, D)\n",
        "\n",
        "        B, T, D = x.size()\n",
        "        head_dim = D // self.num_heads  # Ensure D is divisible by num_heads\n",
        "\n",
        "        # Compute gating weights.\n",
        "        # self.gate should output a tensor of shape (B, T, num_heads*2)\n",
        "        gate_out = self.gate(x)  # Shape: (B, T, num_heads*2)\n",
        "        # Reshape to (B, T, num_heads, 2) where last dim holds [local_gate, global_gate]\n",
        "        gates = gate_out.view(B, T, self.num_heads, 2)\n",
        "        # Unbind the last dimension into two tensors\n",
        "        local_gate = gates[..., 0]  # Shape: (B, T, num_heads)\n",
        "        global_gate = gates[..., 1]  # Shape: (B, T, num_heads)\n",
        "\n",
        "        # Reshape attention outputs to split heads: (B, T, num_heads, head_dim)\n",
        "        local_out_heads = local_out.view(B, T, self.num_heads, head_dim)\n",
        "        global_out_heads = global_out.view(B, T, self.num_heads, head_dim)\n",
        "\n",
        "        # Ensure the gate tensors have an extra dimension for broadcasting: (B, T, num_heads, 1)\n",
        "        local_gate = local_gate.unsqueeze(-1)\n",
        "        global_gate = global_gate.unsqueeze(-1)\n",
        "\n",
        "        # Element-wise multiply each head output by its corresponding gate weight\n",
        "        combined = local_out_heads * local_gate + global_out_heads * global_gate\n",
        "        # Reshape back to (B, T, D)\n",
        "        combined = combined.view(B, T, D)\n",
        "        return self.out_proj(combined)\n"
      ],
      "metadata": {
        "id": "_yfLtbvD56Yl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############## # Code Block 3: Custom GPT-2 Model ##############\n",
        "class SparseGPT2(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.wte = nn.Embedding(config.vocab_size, config.hidden_size)\n",
        "        self.wpe = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "        self.drop = nn.Dropout(config.embd_pdrop)\n",
        "\n",
        "        # Create native sparse attention layer with correct parameters\n",
        "        sparse_config = SPARSE_CONFIG.copy()\n",
        "        sparse_config[\"dim\"] = config.hidden_size\n",
        "        sparse_config[\"compress_block_sliding_stride\"] = 2\n",
        "        self.sparse_attn = SparseAttention(**sparse_config)\n",
        "\n",
        "        self.h = nn.ModuleList(\n",
        "            [\n",
        "                nn.ModuleDict(\n",
        "                    {\n",
        "                        \"attn\": self.sparse_attn,\n",
        "                        \"ln_1\": nn.LayerNorm(config.hidden_size),\n",
        "                        \"mlp\": nn.Sequential(\n",
        "                            nn.Linear(config.hidden_size, 4 * config.hidden_size),\n",
        "                            nn.GELU(),\n",
        "                            nn.Linear(4 * config.hidden_size, config.hidden_size),\n",
        "                        ),\n",
        "                        \"ln_2\": nn.LayerNorm(config.hidden_size),\n",
        "                    }\n",
        "                )\n",
        "                for _ in range(config.num_hidden_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.ln_f = nn.LayerNorm(config.hidden_size)\n",
        "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        B, T = input_ids.size()\n",
        "        pos_ids = torch.arange(T, device=DEVICE).unsqueeze(0)\n",
        "\n",
        "        x = self.drop(self.wte(input_ids) + self.wpe(pos_ids))\n",
        "\n",
        "        attn_out = torch.zeros_like(x)\n",
        "\n",
        "        for block in self.h:\n",
        "            # Apply layer norm before attention\n",
        "            normed_x = block[\"ln_1\"](x)\n",
        "            attention_result = block[\"attn\"](normed_x)\n",
        "\n",
        "            # Apply sparse attention and handle tuple output\n",
        "            # attn_output = block[\"attn\"](normed_x)\n",
        "            if attention_result is not None:\n",
        "                attn_out = attention_result\n",
        "            # # If attn_output is a tuple, take the first element (the main output)\n",
        "            # if isinstance(attn_output, tuple):\n",
        "            #     attn_out = attn_output[0]\n",
        "            # else:\n",
        "            #     attn_out = attn_output\n",
        "\n",
        "            # Apply mask after attention if provided\n",
        "            if attention_mask is not None:\n",
        "                attn_out = attn_out * attention_mask.unsqueeze(-1)\n",
        "\n",
        "            x = x + attn_out\n",
        "            x = x + block[\"mlp\"](block[\"ln_2\"](x))\n",
        "\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "        return logits  # Return only the logits, not a tuple\n"
      ],
      "metadata": {
        "id": "mTdSWwDE53Nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############## # Code Block 4: Training Setup ##############\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Reference model\n",
        "ref_model = GPT2LMHeadModel.from_pretrained(MODEL_NAME).to(DEVICE)\n",
        "ref_model.eval()\n",
        "\n",
        "# Custom model with native sparse attention\n",
        "cust_config = GPT2Config.from_pretrained(MODEL_NAME)\n",
        "cust_model = SparseGPT2(cust_config).to(DEVICE)\n",
        "\n",
        "# Initialize with pretrained weights\n",
        "pretrained_state_dict = ref_model.state_dict()\n",
        "cust_model.load_state_dict(pretrained_state_dict, strict=False)\n",
        "\n",
        "# Use a lower learning rate for fine-tuning\n",
        "optimizer = torch.optim.AdamW(cust_model.parameters(), lr=1e-3)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n"
      ],
      "metadata": {
        "id": "Vq-5THYE5zpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############## # Code Block 5: Training Loop ##############\n",
        "class UsageTracker:\n",
        "    def __init__(self):\n",
        "        self.start_time = None\n",
        "        self.start_cpu_time = None\n",
        "        self.train_stats = {\n",
        "            \"epoch\": [], \"step\": [],\n",
        "            \"wall_time_s\": [], \"cpu_time_s\": [],\n",
        "            \"cpu_mem_mb\": [], \"gpu_alloc_mb\": [], \"gpu_reserved_mb\": []\n",
        "        }\n",
        "        self.infer_stats = {\n",
        "            \"max_token_length\": [],\n",
        "            \"wall_time_s\": [], \"cpu_time_s\": [],\n",
        "            \"cpu_mem_mb\": [], \"gpu_alloc_mb\": [], \"gpu_reserved_mb\": []\n",
        "        }\n",
        "\n",
        "    def start_tracking(self):\n",
        "        self.start_time = time.time()\n",
        "        self.start_cpu_time = time.process_time()\n",
        "\n",
        "    def stop_tracking(self, is_training, epoch=None, step=None, max_token_length=None):\n",
        "        if self.start_time is None or self.start_cpu_time is None:\n",
        "            raise ValueError(\"Tracking not started. Call start_tracking() first.\")\n",
        "\n",
        "        end_time = time.time()\n",
        "        end_cpu_time = time.process_time()\n",
        "\n",
        "        proc = psutil.Process(os.getpid())\n",
        "        cpu_mem = proc.memory_info().rss / (1024**2)\n",
        "        if torch.cuda.is_available():\n",
        "            gpu_alloc    = torch.cuda.memory_allocated(DEVICE) / (1024**2)\n",
        "            gpu_reserved = torch.cuda.memory_reserved(DEVICE) / (1024**2)\n",
        "        else:\n",
        "            gpu_alloc = gpu_reserved = 0.0\n",
        "\n",
        "        obj = self.train_stats if is_training else self.infer_stats\n",
        "        obj[\"wall_time_s\"].append(end_time - self.start_time)\n",
        "        obj[\"cpu_time_s\"].append(end_cpu_time - self.start_cpu_time)\n",
        "        obj[\"cpu_mem_mb\"].append(cpu_mem)\n",
        "        obj[\"gpu_alloc_mb\"].append(gpu_alloc)\n",
        "        obj[\"gpu_reserved_mb\"].append(gpu_reserved)\n",
        "        if is_training:\n",
        "            obj[\"epoch\"].append(epoch + 1)\n",
        "            obj[\"step\"].append(step + 1)\n",
        "        else:\n",
        "            obj[\"max_token_length\"].append(max_token_length)\n",
        "\n",
        "        self.start_time = None\n",
        "        self.start_cpu_time = None\n",
        "\n",
        "    def plot(self):\n",
        "        df_train = pd.DataFrame(self.train_stats)\n",
        "        df_inf   = pd.DataFrame(self.infer_stats)\n",
        "\n",
        "        # --- Training plots (line plots vs. step) ---\n",
        "        for metric in [\"wall_time_s\", \"cpu_time_s\", \"cpu_mem_mb\", \"gpu_alloc_mb\", \"gpu_reserved_mb\"]:\n",
        "            plt.figure(figsize=(8, 4))\n",
        "            plt.plot(df_train[\"step\"], df_train[metric])\n",
        "            plt.xlabel(\"Training Step\")\n",
        "            plt.ylabel(metric.replace(\"_\", \" \").title())\n",
        "            plt.title(f\"{metric.replace('_', ' ').title()} During Training\")\n",
        "            plt.show()\n",
        "\n",
        "        # # --- Inference plots (bar plots vs. call index) ---\n",
        "        # for metric in [\"wall_time_s\", \"cpu_time_s\", \"cpu_mem_mb\", \"gpu_alloc_mb\", \"gpu_reserved_mb\"]:\n",
        "        #     plt.figure(figsize=(8, 4))\n",
        "        #     plt.bar(range(len(df_inf)), df_inf[metric])\n",
        "        #     plt.xlabel(\"Inference Call #\")\n",
        "        #     plt.ylabel(metric.replace(\"_\", \" \").title())\n",
        "        #     plt.title(f\"{metric.replace('_', ' ').title()} Per Inference\")\n",
        "        #     plt.show()\n",
        "\n",
        "        prompt_lengths = self.infer_stats[\"max_token_length\"]\n",
        "\n",
        "        # --- Inference plots (bar plots vs. token length) ---\n",
        "        for metric in [\"wall_time_s\", \"cpu_time_s\", \"cpu_mem_mb\", \"gpu_alloc_mb\", \"gpu_reserved_mb\"]:\n",
        "            plt.figure(figsize=(8, 4))\n",
        "            plt.scatter(prompt_lengths, self.infer_stats[metric])\n",
        "            plt.xlabel(\"Prompt Token Length\")\n",
        "            plt.ylabel(metric.replace(\"_\", \" \").title())\n",
        "            plt.title(f\"{metric.replace('_', ' ').title()} vs. Prompt Length\")\n",
        "            plt.show()\n",
        "\n",
        "def kl_divergence_loss(logits_custom, logits_ref, mask, temperature):\n",
        "    # 1) soften both distributions by T\n",
        "    logp_c = F.log_softmax(logits_custom / temperature, dim=-1)\n",
        "    p_r   = F.softmax(   logits_ref.detach() / temperature, dim=-1)\n",
        "\n",
        "    # 2) perâ€‘token KL\n",
        "    kl = (p_r * (p_r.log() - logp_c)).sum(-1)  # (B, L)\n",
        "\n",
        "    # 3) average over real tokens and reâ€‘scale by T^2\n",
        "    return (kl * mask).sum() / mask.sum() * (temperature * temperature)\n",
        "\n",
        "def train_step(batch, epoch, step):\n",
        "    tracker.start_tracking()\n",
        "\n",
        "    inputs = batch.to(DEVICE)\n",
        "    attention_mask = (inputs != tokenizer.pad_token_id).float()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        ref_outputs = ref_model(inputs, attention_mask=attention_mask)\n",
        "        if isinstance(ref_outputs, tuple):\n",
        "            ref_logits = ref_outputs[0]\n",
        "        else:\n",
        "            ref_logits = ref_outputs.logits  # Extract logits from the output object\n",
        "\n",
        "    cust_outputs = cust_model(inputs, attention_mask=attention_mask)\n",
        "    if isinstance(cust_outputs, tuple):\n",
        "        cust_logits = cust_outputs[0]\n",
        "    else:\n",
        "        cust_logits = cust_outputs\n",
        "\n",
        "    # Use KL divergence loss with temperature\n",
        "    temperature = 0.7\n",
        "    loss = F.kl_div(\n",
        "        F.log_softmax(cust_logits / temperature, dim=-1),\n",
        "        F.softmax(ref_logits / temperature, dim=-1).detach(),\n",
        "        reduction=\"sum\"\n",
        "    ) * (temperature**2)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(cust_model.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "\n",
        "    tracker.stop_tracking(is_training=True, epoch=epoch, step=step)\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "def train_epoch(loader, epoch):\n",
        "    cust_model.train()\n",
        "    total_loss = 0\n",
        "    loss_vals = []\n",
        "    for i, batch in tqdm(enumerate(loader), desc=\"Training\"):\n",
        "        loss = train_step(batch, epoch, i)\n",
        "        total_loss += loss\n",
        "        loss_vals.append(loss)\n",
        "    return total_loss / len(loader), loss_vals"
      ],
      "metadata": {
        "id": "ySu3QUz05vWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############## # Code Block 6: Generation & Evaluation ##############\n",
        "def generate(model, tokenizer, prompt, max_length=50, temperature=0.7, top_k=50, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=False).to(device)\n",
        "    input_ids = input_ids[:1]  # Ensure we only have one batch dimension\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids)\n",
        "            logits = outputs.logits if hasattr(outputs, \"logits\") else outputs\n",
        "\n",
        "            # Get logits for the last token\n",
        "            if logits.dim() == 3:  # Standard shape [batch, seq, vocab]\n",
        "                next_token_logits = logits[:, -1, :] / temperature\n",
        "            elif logits.dim() == 2:  # If somehow we got [batch*seq, vocab]\n",
        "                # We only care about the last token's logits\n",
        "                next_token_logits = logits[-1:, :] / temperature\n",
        "            else:\n",
        "                raise ValueError(f\"Unexpected logits shape: {logits.shape}\")\n",
        "\n",
        "            # Apply top-k filtering\n",
        "            top_k_logits, top_k_indices = torch.topk(next_token_logits, k=top_k, dim=-1)\n",
        "\n",
        "            # Convert to probabilities\n",
        "            probs = F.softmax(top_k_logits, dim=-1)\n",
        "\n",
        "            # Sample next token index from top-k logits\n",
        "            next_token_idx = torch.multinomial(probs, num_samples=1)\n",
        "            next_token = top_k_indices.gather(1, next_token_idx)\n",
        "\n",
        "            # Ensure next_token has shape [1, 1]\n",
        "            next_token = next_token[-1:, :]  # Take only the last row if needed\n",
        "\n",
        "            # Concatenate to input_ids\n",
        "            input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
        "\n",
        "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "4hSVJ-2d5rxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############## # Code Block 7: Dataset Preparation ##############\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "class WikiDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, seq_len):\n",
        "        self.samples = []\n",
        "        for text in texts:\n",
        "            # Tokenize each text separately, without adding special tokens\n",
        "            token_ids = tokenizer.encode(text, add_special_tokens=False)\n",
        "            # Split token_ids into chunks of length seq_len\n",
        "            for i in range(0, len(token_ids), seq_len):\n",
        "                chunk = token_ids[i : i + seq_len]\n",
        "                # Only add full chunks to avoid very short sequences\n",
        "                if len(chunk) == seq_len:\n",
        "                    self.samples.append(torch.tensor(chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx]\n",
        "\n",
        "\n",
        "# Load a small subset (e.g., 1% of the train split) of WikiText data\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train[:100%]\")\n",
        "texts = dataset[\"text\"]\n",
        "wiki_dataset = WikiDataset(texts, tokenizer, SEQ_LEN)\n",
        "\n",
        "# Create a DataLoader for training\n",
        "train_loader = DataLoader(wiki_dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "id": "VAzIBY4g5oIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############## # Code Block 8: Training Execution ##############\n",
        "tracker = UsageTracker()\n",
        "\n",
        "CHECKPOINT_DIR = '/content/drive/MyDrive/attention_optimization/model_checkpoints'\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "start_epoch = 6\n",
        "ckpt_path = f\"/content/drive/MyDrive/attention_optimization/model_checkpoints/ckpt_epoch_{start_epoch}.pth\"\n",
        "checkpoint = torch.load(ckpt_path, map_location=DEVICE)\n",
        "\n",
        "cust_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "start_epoch = checkpoint['epoch']\n",
        "loss_vals = checkpoint['loss_vals']\n",
        "# val_loss_vals = checkpoint['val_loss_vals']\n",
        "# all_alphas = checkpoint['all_alphas']\n",
        "\n",
        "cust_model.train()\n",
        "\n",
        "print(f\"Resumed from epoch {start_epoch}\")\n",
        "\n",
        "for epoch in range(start_epoch + 1, NUM_EPOCHS):\n",
        "\n",
        "    avg_loss, loss_vals = train_epoch(train_loader, epoch)\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} - Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Generate sample text after each epoch\n",
        "    if (epoch + 1) % 2 == 0:  # Generate every 2 epochs\n",
        "        print(\"\\nGenerating sample text:\")\n",
        "        prompt = \"Artificial intelligence\"\n",
        "        print(\"Reference:\", generate(ref_model, tokenizer, prompt, temperature=0.7, top_k=50))\n",
        "        print(\"Custom:\", generate(cust_model, tokenizer, prompt, temperature=0.7, top_k=50))\n",
        "        print(\"\\n\")\n",
        "\n",
        "    # checkpoint model vars\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        checkpoint = {\n",
        "            'epoch': epoch + 1, # next epoch to start from\n",
        "            'model_state_dict': cust_model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss_vals': loss_vals,\n",
        "            # 'val_loss_vals': val_loss_vals,\n",
        "            # 'all_alphas': all_alphas\n",
        "        }\n",
        "        torch.save(checkpoint, os.path.join(CHECKPOINT_DIR, f'ckpt_epoch_{epoch+1}.pth'))\n",
        "        print(f\"Saved checkpoint for epoch {epoch+1}\")\n",
        "\n",
        "# Final generation comparison\n",
        "prompt = \"Artificial intelligence\"\n",
        "print(\"\\nFinal generation comparison:\")\n",
        "print(\"Reference:\", generate(ref_model, tokenizer, prompt, temperature=0.7, top_k=50))\n",
        "print(\"Custom:\", generate(cust_model, tokenizer, prompt, temperature=0.7, top_k=50))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rh5v-LPzWihD",
        "outputId": "8bbb0e42-da32-435f-ac12-344bb9df3e76"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resumed from epoch 6\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 2571it [09:09,  4.68it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/20 - Average Loss: 207.8354\n",
            "\n",
            "Generating sample text:\n",
            "Reference: Artificial intelligence is a big step forward in this area of research.\n",
            "\n",
            "You can connect to a human's brain after they're born, and even see a person's face from a distance, but your sense of what they're wearing and hearing will not be\n",
            "Custom: Artificial intelligence as the first to be built in by the United States Environmental Protection Agency (BOTOTIIANOSANAR ) as the first major C EEAED ETSSTOSANDAOSOSIM in 2013 and 2007 BC BC-\n",
            "\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 2571it [09:05,  4.72it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/20 - Average Loss: 188.6792\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 2571it [09:04,  4.72it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/20 - Average Loss: 174.0925\n",
            "\n",
            "Generating sample text:\n",
            "Reference: Artificial intelligence can now take over the world, with the aim of bringing about the ultimate goal of global economic and political change.[1]\n",
            "\n",
            "The research has also been performed by the UK Economic and Social Research Council (ESRC), which has been funded by\n",
            "Custom: Artificial intelligence, you.\n",
            "\n",
            "The and 2: the user-key, and & and MacLeodfish @ 3- @-:-Loids on the Space One. The Effect-1 (H ) )\n",
            "\n",
            "\n",
            "-S ( OAS\n",
            "\n",
            "\n",
            "Saved checkpoint for epoch 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 2571it [09:09,  4.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/20 - Average Loss: 161.1676\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 2571it [09:08,  4.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/20 - Average Loss: 149.6092\n",
            "\n",
            "Generating sample text:\n",
            "Reference: Artificial intelligence (AI) will be developed in earnest, as our technology's ability to understand what humans are saying is a fundamental skill, and will become more and more important, as AI becomes more and more important to the business world. This will lead to the\n",
            "Custom: Artificial intelligence report was also investigated as evidence suggests that this is a \"clear-dorf model with another three types-linked series. cit. The New York Times reporter had \"found a high-tonstoneton'sululine. (17 ), Valley\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 2571it [09:03,  4.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/20 - Average Loss: 139.0573\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 2571it [08:58,  4.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/20 - Average Loss: 129.5528\n",
            "\n",
            "Generating sample text:\n",
            "Reference: Artificial intelligence (AI) will be the next step in this conversation. The next step is to see if it can handle an infinite number of tasks for which it has no real experience. The next step is to see if AI can create a better or worse AI\n",
            "Custom: Artificial intelligence policy. the public press conference to explain that the media group have been looking at the government'sians, and to those who are actually found in that they are more than in the case cases and in the media, and in the media coverage program news\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 2571it [09:09,  4.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/20 - Average Loss: 120.7946\n",
            "Saved checkpoint for epoch 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 2571it [09:08,  4.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/20 - Average Loss: 112.8696\n",
            "\n",
            "Generating sample text:\n",
            "Reference: Artificial intelligence has been used to train animals for more than a century, and most of those trained to do so have been trained to be intelligent. It has been known for some time that humans are \"super intelligent\" and that many other primates have been created \"\n",
            "Custom: Artificial intelligence needs a very large percentage of the food. or other people.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "- #6 CXTIE.I is used in as a whole, you, it, you . [ 1 , if a whole offer way\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 2571it [09:07,  4.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/20 - Average Loss: 105.5966\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 2571it [09:07,  4.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/20 - Average Loss: 99.0282\n",
            "\n",
            "Generating sample text:\n",
            "Reference: Artificial intelligence (AI) is the future of computing, and it will revolutionize the way we think about the world. Artificial intelligence is a major innovation in today's technological world.\n",
            "\n",
            "AI is also a way for our society to live and work without having\n",
            "Custom: Artificial intelligence-0-1A-1, @980V/ (AT,@ or @,@ 16491525-/-W,@ 4,@ 3,@ 3888M /)-100@2 @-,/ )\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 2571it [09:07,  4.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/20 - Average Loss: 93.2158\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 2571it [09:07,  4.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/20 - Average Loss: 88.0451\n",
            "\n",
            "Generating sample text:\n",
            "Reference: Artificial intelligence. The process is described by a group of researchers at the University of California, Berkeley. The team aims to use machine learning to better understand how robots will interact with humans, with the aim of developing tools that enable social networks to communicate with each other\n",
            "Custom: Artificial intelligence service station CCTV. A. (J. . CPT, M. D. (director , M6HOXC/GECCTRM and PEGX2XXXXC.TUYITMC.\n",
            "\n",
            "\n",
            "Saved checkpoint for epoch 20\n",
            "\n",
            "Final generation comparison:\n",
            "Reference: Artificial intelligence is already one of the top 10 technologies in the business, but the number is growing every year.\n",
            "\n",
            "The company has now confirmed that it is the first company to offer the full range of artificial intelligence and artificial intelligence tools.\n",
            "\n",
            "The firm\n",
            "Custom: Artificial intelligence agents have long been able-tearicke , and may exist. it'seticizedization , where there is a substantial number of diseases, and may be a very limited number of waysisedisedised (especially , pumptusob\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad7z6_03TOs2"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "We have demonstrated:\n",
        "1) Loading a reference GPT-2 model from Hugging Face.\n",
        "2) Creating a custom GPT-2-like model with a simplified \"last-5-tokens\" attention mechanism.\n",
        "3) Setting up a dataset and training loop that optimizes the custom model to match the reference distribution via KL-divergence.\n",
        "4) Showed a simple comparison of generated text from both models.\n",
        "\n",
        "This notebook is purely for demonstration and educational purposes, and many improvements could be made:\n",
        "- More elaborate data loading\n",
        "- Proper scheduling, regularization\n",
        "- Additional GPT-2 intricacies (like caching attention states, etc.)\n",
        "- More advanced generation strategies (beam search, top-k, top-p, etc.)\n",
        "\n",
        "But this entire workflow shows how one could begin to experiment with custom attention\n",
        "mechanisms and align them to a known distribution via KL divergence."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}